diff --git a/pom.xml b/pom.xml
index a86595bfd..9a37490a9 100644
--- a/pom.xml
+++ b/pom.xml
@@ -751,6 +751,17 @@
         <artifactId>leveldbjni-all</artifactId>
         <version>1.8</version>
       </dependency>
+
+      <dependency>
+        <groupId>com.aliyun.emr</groupId>
+        <artifactId>ess-shuffle-manager-tez</artifactId>
+        <version>tez-SNAPSHOT</version>
+      </dependency>
+      <dependency>
+        <groupId>com.aliyun.emr</groupId>
+        <artifactId>ess-client-java</artifactId>
+        <version>tez-SNAPSHOT</version>
+      </dependency>
     </dependencies>
   </dependencyManagement>
 
diff --git a/tez-api/src/main/java/org/apache/tez/dag/api/EntityDescriptor.java b/tez-api/src/main/java/org/apache/tez/dag/api/EntityDescriptor.java
index 13d4a93f0..ce06a759a 100644
--- a/tez-api/src/main/java/org/apache/tez/dag/api/EntityDescriptor.java
+++ b/tez-api/src/main/java/org/apache/tez/dag/api/EntityDescriptor.java
@@ -56,6 +56,10 @@ public abstract class EntityDescriptor<T extends EntityDescriptor<T>> implements
     this.className = className;
   }
 
+  public void setClassName(String className) {
+    this.className = className;
+  }
+
   public UserPayload getUserPayload() {
     return userPayload;
   }
diff --git a/tez-api/src/main/java/org/apache/tez/dag/api/TezConfiguration.java b/tez-api/src/main/java/org/apache/tez/dag/api/TezConfiguration.java
index a206bb73d..ad977b5fd 100644
--- a/tez-api/src/main/java/org/apache/tez/dag/api/TezConfiguration.java
+++ b/tez-api/src/main/java/org/apache/tez/dag/api/TezConfiguration.java
@@ -2020,4 +2020,7 @@ public class TezConfiguration extends Configuration {
   @ConfigurationProperty
   public static final String TEZ_JOB_FS_SERVERS_TOKEN_RENEWAL_EXCLUDE = "tez.job.fs-servers.token-renewal.exclude";
 
+  public static final String TEZ_RSS_PARTITION_THRESHOLD = "tez.rss.partition.threshold";
+  public static final int TEZ_RSS_PARTITION_THRESHOLD_DEFAULT = 20;
+
 }
diff --git a/tez-api/src/main/java/org/apache/tez/runtime/api/AbstractLogicalInput.java b/tez-api/src/main/java/org/apache/tez/runtime/api/AbstractLogicalInput.java
index a97f3fa6d..ba6e96e51 100644
--- a/tez-api/src/main/java/org/apache/tez/runtime/api/AbstractLogicalInput.java
+++ b/tez-api/src/main/java/org/apache/tez/runtime/api/AbstractLogicalInput.java
@@ -85,4 +85,10 @@ public abstract class AbstractLogicalInput implements LogicalInput, LogicalInput
   public float getProgress() throws ProgressFailedException, InterruptedException {
     return 0.0f;
   }
+
+  public void setNumInputs(int numInputs) {}
+
+  public long getReservedMemory() {
+    return 0;
+  }
 }
diff --git a/tez-api/src/main/java/org/apache/tez/runtime/api/AbstractLogicalOutput.java b/tez-api/src/main/java/org/apache/tez/runtime/api/AbstractLogicalOutput.java
index f36b7f2ec..581ceb6fd 100644
--- a/tez-api/src/main/java/org/apache/tez/runtime/api/AbstractLogicalOutput.java
+++ b/tez-api/src/main/java/org/apache/tez/runtime/api/AbstractLogicalOutput.java
@@ -73,4 +73,8 @@ public abstract class AbstractLogicalOutput implements LogicalOutput, LogicalOut
   public final OutputContext getContext() {
     return outputContext;
   }
+
+  public long getReservedMemory() {
+    return 0;
+  }
 }
diff --git a/tez-api/src/main/java/org/apache/tez/runtime/api/ProcessorContext.java b/tez-api/src/main/java/org/apache/tez/runtime/api/ProcessorContext.java
index acb2a57da..b4bfc773b 100644
--- a/tez-api/src/main/java/org/apache/tez/runtime/api/ProcessorContext.java
+++ b/tez-api/src/main/java/org/apache/tez/runtime/api/ProcessorContext.java
@@ -117,4 +117,8 @@ public interface ProcessorContext extends TaskContext {
    * @throws InterruptedException
    */
   public boolean waitForAllInputsReady(Collection<Input> inputs, long timeoutMillis) throws InterruptedException;
+
+  public void decrementAvailableMemory(long delta);
+
+  public void incrementAvailableMemory(long delta);
 }
diff --git a/tez-common/pom.xml b/tez-common/pom.xml
index fffe521ad..16abcbab4 100644
--- a/tez-common/pom.xml
+++ b/tez-common/pom.xml
@@ -71,6 +71,12 @@
       <artifactId>junit</artifactId>
       <scope>test</scope>
     </dependency>
+
+    <dependency>
+      <groupId>com.aliyun.emr</groupId>
+      <artifactId>ess-shuffle-manager-tez</artifactId>
+      <scope>provided</scope>
+    </dependency>
   </dependencies>
 
   <build>
diff --git a/tez-common/src/main/java/org/apache/tez/runtime/ess/RssShuffleManagerFactory.java b/tez-common/src/main/java/org/apache/tez/runtime/ess/RssShuffleManagerFactory.java
new file mode 100644
index 000000000..388c7802e
--- /dev/null
+++ b/tez-common/src/main/java/org/apache/tez/runtime/ess/RssShuffleManagerFactory.java
@@ -0,0 +1,40 @@
+package org.apache.tez.runtime.ess;
+
+import com.aliyun.emr.ess.common.EssConf;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.tez.common.TezUtils;
+import org.apache.tez.dag.api.UserPayload;
+import org.apache.tez.shuffle.ess.EssShuffleManager;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.util.Map;
+
+public class RssShuffleManagerFactory {
+    private static final Logger LOG = LoggerFactory.getLogger(RssShuffleManagerFactory.class);
+    private static EssShuffleManager _essShuffleManager;
+
+    public synchronized static EssShuffleManager getEssShuffleManager(String appId, Configuration conf) {
+        if (_essShuffleManager == null) {
+            EssConf essConf = EssShuffleManager.fromTezConf(conf);
+            LOG.info("Master address " + essConf.get("ess.master.address"));
+            _essShuffleManager = new EssShuffleManager(appId, essConf);
+        }
+        return _essShuffleManager;
+    }
+
+    public static boolean isRssEnabled(UserPayload userPayload) {
+        try {
+            Configuration conf = TezUtils.createConfFromUserPayload(userPayload);
+            return isRssEnabled(conf);
+        } catch (IOException e) {
+            LOG.warn("Parse failed for user payload when check rss enabled or not!", e);
+        }
+        return false;
+    }
+
+    public static boolean isRssEnabled(Configuration conf) {
+        return conf.getBoolean("tez.runtime.ess.enabled", false);
+    }
+}
diff --git a/tez-dag/pom.xml b/tez-dag/pom.xml
index 0ca71a69e..7918d54af 100644
--- a/tez-dag/pom.xml
+++ b/tez-dag/pom.xml
@@ -157,6 +157,11 @@
       <artifactId>servlet-api</artifactId>
       <scope>compile</scope>
     </dependency>
+
+    <dependency>
+      <groupId>com.aliyun.emr</groupId>
+      <artifactId>ess-shuffle-manager-tez</artifactId>
+    </dependency>
   </dependencies>
 
   <build>
diff --git a/tez-dag/src/main/java/org/apache/tez/dag/app/dag/impl/DAGImpl.java b/tez-dag/src/main/java/org/apache/tez/dag/app/dag/impl/DAGImpl.java
index 8cb39a2c7..5487c2999 100644
--- a/tez-dag/src/main/java/org/apache/tez/dag/app/dag/impl/DAGImpl.java
+++ b/tez-dag/src/main/java/org/apache/tez/dag/app/dag/impl/DAGImpl.java
@@ -44,6 +44,7 @@ import org.apache.commons.lang.StringUtils;
 import org.apache.commons.lang.exception.ExceptionUtils;
 import org.apache.tez.common.TezUtilsInternal;
 import org.apache.tez.common.counters.LimitExceededException;
+import org.apache.tez.dag.api.*;
 import org.apache.tez.dag.app.dag.event.DAGEventTerminateDag;
 import org.apache.tez.dag.app.dag.event.DiagnosableEvent;
 import org.apache.tez.state.OnStateChangedCallback;
@@ -66,14 +67,6 @@ import org.apache.tez.common.ATSConstants;
 import org.apache.tez.common.ReflectionUtils;
 import org.apache.tez.common.counters.DAGCounter;
 import org.apache.tez.common.counters.TezCounters;
-import org.apache.tez.dag.api.DagTypeConverters;
-import org.apache.tez.dag.api.EdgeProperty;
-import org.apache.tez.dag.api.InputDescriptor;
-import org.apache.tez.dag.api.Scope;
-import org.apache.tez.dag.api.TezConfiguration;
-import org.apache.tez.dag.api.TezException;
-import org.apache.tez.dag.api.TezUncheckedException;
-import org.apache.tez.dag.api.VertexLocationHint;
 import org.apache.tez.dag.api.client.DAGStatusBuilder;
 import org.apache.tez.dag.api.client.ProgressBuilder;
 import org.apache.tez.dag.api.client.StatusGetOpts;
@@ -173,6 +166,7 @@ public class DAGImpl implements org.apache.tez.dag.app.dag.DAG,
   private AtomicBoolean aborted = new AtomicBoolean(false);
   private AtomicBoolean commitCanceled = new AtomicBoolean(false);
   boolean commitAllOutputsOnSuccess = true;
+  private int rssPartitionThreshold = 0;
 
   @VisibleForTesting
   DAGScheduler dagScheduler;
@@ -1519,6 +1513,11 @@ public class DAGImpl implements org.apache.tez.dag.app.dag.DAG,
         TezConfiguration.TEZ_AM_COMMIT_ALL_OUTPUTS_ON_DAG_SUCCESS,
         TezConfiguration.TEZ_AM_COMMIT_ALL_OUTPUTS_ON_DAG_SUCCESS_DEFAULT);
 
+    rssPartitionThreshold = dagConf.getInt(
+        TezConfiguration.TEZ_RSS_PARTITION_THRESHOLD,
+        TezConfiguration.TEZ_RSS_PARTITION_THRESHOLD_DEFAULT
+    );
+
     // If we have no vertices, fail the dag
     numVertices = getJobPlan().getVertexCount();
     if (numVertices == 0) {
@@ -1711,6 +1710,15 @@ public class DAGImpl implements org.apache.tez.dag.app.dag.DAG,
       edge.setSourceVertex(inVertex);
       edge.setDestinationVertex(vertex);
       inVertices.put(inVertex, edge);
+
+      LOG.info("total tasks " + vertex.getTotalTasks() + ", threshold " + dag.rssPartitionThreshold);
+      if (vertex.getTotalTasks() < dag.rssPartitionThreshold) {
+        InputDescriptor inputDescriptor = edge.getEdgeProperty().getEdgeDestination();
+        OutputDescriptor outputDescriptor = edge.getEdgeProperty().getEdgeSource();
+        LOG.info("original class name " + inputDescriptor.getClassName() + ", " + outputDescriptor.getClassName());
+        inputDescriptor.setClassName(inputDescriptor.getClassName().replace("Rss", ""));
+        outputDescriptor.setClassName(outputDescriptor.getClassName().replace("Rss", ""));
+      }
     }
 
     for(String outEdgeId : vertexPlan.getOutEdgeIdList()){
@@ -1720,6 +1728,15 @@ public class DAGImpl implements org.apache.tez.dag.app.dag.DAG,
       edge.setSourceVertex(vertex);
       edge.setDestinationVertex(outVertex);
       outVertices.put(outVertex, edge);
+
+      LOG.info("out total tasks " + outVertex.getTotalTasks() + ", threshold " + dag.rssPartitionThreshold);
+      if (outVertex.getTotalTasks() < dag.rssPartitionThreshold) {
+        InputDescriptor inputDescriptor = edge.getEdgeProperty().getEdgeDestination();
+        OutputDescriptor outputDescriptor = edge.getEdgeProperty().getEdgeSource();
+        LOG.info("original class name " + inputDescriptor.getClassName() + ", " + outputDescriptor.getClassName());
+        inputDescriptor.setClassName(inputDescriptor.getClassName().replace("Rss", ""));
+        outputDescriptor.setClassName(outputDescriptor.getClassName().replace("Rss", ""));
+      }
     }
 
     vertex.setInputVertices(inVertices);
diff --git a/tez-dag/src/main/java/org/apache/tez/dag/app/dag/impl/VertexImpl.java b/tez-dag/src/main/java/org/apache/tez/dag/app/dag/impl/VertexImpl.java
index d8ac67026..afe91304d 100644
--- a/tez-dag/src/main/java/org/apache/tez/dag/app/dag/impl/VertexImpl.java
+++ b/tez-dag/src/main/java/org/apache/tez/dag/app/dag/impl/VertexImpl.java
@@ -182,6 +182,7 @@ import org.apache.tez.runtime.api.impl.OutputSpec;
 import org.apache.tez.runtime.api.impl.TaskSpec;
 import org.apache.tez.runtime.api.impl.TaskStatistics;
 import org.apache.tez.runtime.api.impl.TezEvent;
+import org.apache.tez.runtime.ess.RssShuffleManagerFactory;
 import org.apache.tez.state.OnStateChangedCallback;
 import org.apache.tez.state.StateMachineTez;
 import org.slf4j.Logger;
@@ -1714,7 +1715,7 @@ public class VertexImpl implements org.apache.tez.dag.app.dag.Vertex, EventHandl
       @Nullable Map<String, InputSpecUpdate> rootInputSpecUpdate) throws AMUserCodeException {
     setParallelismWrapper(parallelism, locationHint, sourceEdgeProperties, rootInputSpecUpdate, true);
   }
-  
+
   @Override
   public void setParallelism(int parallelism, VertexLocationHint vertexLocationHint,
       Map<String, EdgeManagerPluginDescriptor> sourceEdgeManagers,
@@ -3569,6 +3570,12 @@ public class VertexImpl implements org.apache.tez.dag.app.dag.Vertex, EventHandl
       if(state == VertexState.RUNNING && forceTransitionToKillWait){
         return VertexState.TERMINATING;
       }
+      // TODO correctly call unregister shuffle
+//      if (state != VertexState.RUNNING) {
+//        LOG.info("appId: " + vertex.getAppContext().getApplicationID().toString() + "\n" +
+//            "vertexName: " + vertex.getName());
+//        vertex.getEssShuffleManager().unregisterShuffle(vertex.getName().hashCode(), true);
+//      }
 
       return state;
     }
diff --git a/tez-runtime-internals/src/main/java/org/apache/tez/runtime/LogicalIOProcessorRuntimeTask.java b/tez-runtime-internals/src/main/java/org/apache/tez/runtime/LogicalIOProcessorRuntimeTask.java
index 0ac916f10..6c231c9f5 100644
--- a/tez-runtime-internals/src/main/java/org/apache/tez/runtime/LogicalIOProcessorRuntimeTask.java
+++ b/tez-runtime-internals/src/main/java/org/apache/tez/runtime/LogicalIOProcessorRuntimeTask.java
@@ -46,8 +46,7 @@ import java.util.concurrent.LinkedBlockingQueue;
 
 import org.apache.commons.lang.exception.ExceptionUtils;
 import org.apache.tez.hadoop.shim.HadoopShim;
-import org.apache.tez.runtime.api.TaskFailureType;
-import org.apache.tez.runtime.api.TaskContext;
+import org.apache.tez.runtime.api.*;
 import org.apache.tez.runtime.api.impl.TezProcessorContextImpl;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -64,24 +63,6 @@ import org.apache.tez.dag.api.TezConfiguration;
 import org.apache.tez.dag.api.TezException;
 import org.apache.tez.dag.api.TezUncheckedException;
 import org.apache.tez.dag.records.TezTaskAttemptID;
-import org.apache.tez.runtime.api.AbstractLogicalIOProcessor;
-import org.apache.tez.runtime.api.ExecutionContext;
-import org.apache.tez.runtime.api.Event;
-import org.apache.tez.runtime.api.Input;
-import org.apache.tez.runtime.api.InputFrameworkInterface;
-import org.apache.tez.runtime.api.LogicalIOProcessor;
-import org.apache.tez.runtime.api.LogicalInput;
-import org.apache.tez.runtime.api.LogicalOutput;
-import org.apache.tez.runtime.api.LogicalOutputFrameworkInterface;
-import org.apache.tez.runtime.api.MergedLogicalInput;
-import org.apache.tez.runtime.api.ObjectRegistry;
-import org.apache.tez.runtime.api.Output;
-import org.apache.tez.runtime.api.OutputFrameworkInterface;
-import org.apache.tez.runtime.api.Processor;
-import org.apache.tez.runtime.api.InputContext;
-import org.apache.tez.runtime.api.MergedInputContext;
-import org.apache.tez.runtime.api.OutputContext;
-import org.apache.tez.runtime.api.ProcessorContext;
 import org.apache.tez.runtime.api.impl.EventMetaData;
 import org.apache.tez.runtime.api.impl.EventMetaData.EventProducerConsumerType;
 import org.apache.tez.runtime.api.impl.GroupInputSpec;
@@ -294,6 +275,21 @@ public class LogicalIOProcessorRuntimeTask extends RuntimeTask {
 
     initialMemoryDistributor.makeInitialAllocations();
 
+    for (LogicalInput input : inputsMap.values()) {
+      if (input instanceof AbstractLogicalInput) {
+        LOG.info("input decrement " + ((AbstractLogicalInput) input).getReservedMemory());
+        processorContext.decrementAvailableMemory(((AbstractLogicalInput) input).getReservedMemory());
+      }
+    }
+
+    for (LogicalOutput output : outputsMap.values()) {
+      // decrement available memory
+      if (output instanceof AbstractLogicalOutput) {
+        LOG.info("output decrement " + ((AbstractLogicalOutput) output).getReservedMemory());
+        processorContext.decrementAvailableMemory(((AbstractLogicalOutput) output).getReservedMemory());
+      }
+    }
+
     LOG.info("Starting Inputs/Outputs");
     int numAutoStarts = 0;
     for (InputSpec inputSpec : inputSpecs) {
@@ -649,6 +645,9 @@ public class LogicalIOProcessorRuntimeTask extends RuntimeTask {
     Input input = ReflectionUtils.createClazzInstance(inputDesc.getClassName(),
         new Class[]{InputContext.class, Integer.TYPE},
         new Object[]{inputContext, inputSpec.getPhysicalEdgeCount()});
+    if (input instanceof AbstractLogicalInput) {
+      ((AbstractLogicalInput) input).setNumInputs(taskSpec.getInputs().size());
+    }
     if (!(input instanceof LogicalInput)) {
       throw new TezUncheckedException(inputDesc.getClass().getName()
           + " is not a sub-type of LogicalInput."
diff --git a/tez-runtime-internals/src/main/java/org/apache/tez/runtime/api/impl/TezProcessorContextImpl.java b/tez-runtime-internals/src/main/java/org/apache/tez/runtime/api/impl/TezProcessorContextImpl.java
index beae69305..e4072b8ad 100644
--- a/tez-runtime-internals/src/main/java/org/apache/tez/runtime/api/impl/TezProcessorContextImpl.java
+++ b/tez-runtime-internals/src/main/java/org/apache/tez/runtime/api/impl/TezProcessorContextImpl.java
@@ -141,6 +141,16 @@ public class TezProcessorContextImpl extends TezTaskContextImpl implements Proce
     return inputReadyTracker.waitForAllInputsReady(inputs, timeoutMillis);
   }
 
+  @Override
+  public void decrementAvailableMemory(long delta) {
+    memAvailable -= delta;
+  }
+
+  @Override
+  public void incrementAvailableMemory(long delta) {
+    memAvailable += delta;
+  }
+
   @Override
   public void close() throws IOException {
     super.close();
diff --git a/tez-runtime-internals/src/main/java/org/apache/tez/runtime/api/impl/TezTaskContextImpl.java b/tez-runtime-internals/src/main/java/org/apache/tez/runtime/api/impl/TezTaskContextImpl.java
index 5a6a40560..8161e1d1e 100644
--- a/tez-runtime-internals/src/main/java/org/apache/tez/runtime/api/impl/TezTaskContextImpl.java
+++ b/tez-runtime-internals/src/main/java/org/apache/tez/runtime/api/impl/TezTaskContextImpl.java
@@ -68,7 +68,7 @@ public abstract class TezTaskContextImpl implements TaskContext, Closeable {
   private volatile ObjectRegistry objectRegistry;
   private final int vertexParallelism;
   private final ExecutionContext ExecutionContext;
-  private final long memAvailable;
+  protected long memAvailable;
   private final TezExecutors sharedExecutor;
 
   @Private
diff --git a/tez-runtime-library/pom.xml b/tez-runtime-library/pom.xml
index 7d5e28b83..252c8b6c3 100644
--- a/tez-runtime-library/pom.xml
+++ b/tez-runtime-library/pom.xml
@@ -101,6 +101,17 @@
       <artifactId>mockito-all</artifactId>
       <scope>test</scope>
     </dependency>
+
+    <dependency>
+      <groupId>com.aliyun.emr</groupId>
+      <artifactId>ess-shuffle-manager-tez</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>com.aliyun.emr</groupId>
+      <artifactId>ess-client-java</artifactId>
+      <scope>provided</scope>
+    </dependency>
   </dependencies>
 
   <build>
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/dag/library/vertexmanager/ShuffleVertexManagerBase.java b/tez-runtime-library/src/main/java/org/apache/tez/dag/library/vertexmanager/ShuffleVertexManagerBase.java
index bb63bd5ce..22d5206d6 100644
--- a/tez-runtime-library/src/main/java/org/apache/tez/dag/library/vertexmanager/ShuffleVertexManagerBase.java
+++ b/tez-runtime-library/src/main/java/org/apache/tez/dag/library/vertexmanager/ShuffleVertexManagerBase.java
@@ -92,6 +92,7 @@ abstract class ShuffleVertexManagerBase extends VertexManagerPlugin {
   private Set<TaskIdentifier> taskWithVmEvents = Sets.newHashSet();
 
   //Track source vertex and its finished tasks
+  private boolean hasRssInput = false;
   private final Map<String, SourceVertexInfo> srcVertexInfo = Maps.newConcurrentMap();
   boolean sourceVerticesScheduled = false;
   @VisibleForTesting
@@ -220,6 +221,10 @@ abstract class ShuffleVertexManagerBase extends VertexManagerPlugin {
     // examine edges after vertex started because until then these may not have been defined
     Map<String, EdgeProperty> inputs = getContext().getInputVertexEdgeProperties();
     for(Map.Entry<String, EdgeProperty> entry : inputs.entrySet()) {
+      if (entry.getValue().getEdgeDestination().getClassName().contains("Rss")) {
+        LOG.info("contains rss! " + entry.getValue().getEdgeDestination().getClassName());
+        hasRssInput = true;
+      }
       srcVertexInfo.put(entry.getKey(), createSourceVertexInfo(entry.getValue(),
           getContext().getVertexNumTasks(getContext().getVertexName())));
       // TODO what if derived class has already called this
@@ -230,6 +235,7 @@ abstract class ShuffleVertexManagerBase extends VertexManagerPlugin {
         bipartiteSources++;
       }
     }
+    LOG.info("contains rss: " + hasRssInput);
     onVertexStartedCheck();
 
     for (VertexStateUpdate stateUpdate : pendingStateUpdates) {
@@ -600,39 +606,52 @@ abstract class ShuffleVertexManagerBase extends VertexManagerPlugin {
     return numTasksToSchedule;
   }
 
-  int getNumOfTasksToSchedule(float minSourceVertexCompletedTaskFraction) {
+  int getNumOfTasksToScheduleForRss(float minSourceVertexCompletedTaskFraction) {
     int numPendingTasks = pendingTasks.size();
     if (numBipartiteSourceTasksCompleted == totalNumBipartiteSourceTasks) {
       LOG.info("All source tasks completed. Ramping up {} remaining tasks" +
           " for vertex: {}", numPendingTasks, getContext().getVertexName());
       return numPendingTasks;
+    } else {
+      return 0;
     }
+  }
 
-    // start scheduling when source tasks completed fraction is more than min.
-    // linearly increase the number of scheduled tasks such that all tasks are
-    // scheduled when source tasks completed fraction reaches max
-    float tasksFractionToSchedule = 1;
-    float percentRange =
-        config.getMaxFraction() - config.getMinFraction();
-    if (percentRange > 0) {
-      tasksFractionToSchedule =
-          (minSourceVertexCompletedTaskFraction -
-              config.getMinFraction()) / percentRange;
+  int getNumOfTasksToSchedule(float minSourceVertexCompletedTaskFraction) {
+    int numPendingTasks = pendingTasks.size();
+    if (numBipartiteSourceTasksCompleted == totalNumBipartiteSourceTasks) {
+      LOG.info("All source tasks completed. Ramping up {} remaining tasks" +
+          " for vertex: {}", numPendingTasks, getContext().getVertexName());
+      return numPendingTasks;
+    } else if (hasRssInput) {
+      return 0;
     } else {
-      // min and max are equal. schedule 100% on reaching min
-      if(minSourceVertexCompletedTaskFraction <
-          config.getMinFraction()) {
-        tasksFractionToSchedule = 0;
+      // start scheduling when source tasks completed fraction is more than min.
+      // linearly increase the number of scheduled tasks such that all tasks are
+      // scheduled when source tasks completed fraction reaches max
+      float tasksFractionToSchedule = 1;
+      float percentRange =
+          config.getMaxFraction() - config.getMinFraction();
+      if (percentRange > 0) {
+        tasksFractionToSchedule =
+            (minSourceVertexCompletedTaskFraction -
+                config.getMinFraction()) / percentRange;
+      } else {
+        // min and max are equal. schedule 100% on reaching min
+        if(minSourceVertexCompletedTaskFraction <
+            config.getMinFraction()) {
+          tasksFractionToSchedule = 0;
+        }
       }
-    }
 
-    tasksFractionToSchedule =
-        Math.max(0, Math.min(1, tasksFractionToSchedule));
+      tasksFractionToSchedule =
+          Math.max(0, Math.min(1, tasksFractionToSchedule));
 
-    // round up to avoid the corner case that single task cannot be scheduled
-    // until src completed fraction reach max
-    return ((int)(Math.ceil(tasksFractionToSchedule * totalTasksToSchedule)) -
-        (totalTasksToSchedule - numPendingTasks));
+      // round up to avoid the corner case that single task cannot be scheduled
+      // until src completed fraction reach max
+      return ((int)(Math.ceil(tasksFractionToSchedule * totalTasksToSchedule)) -
+          (totalTasksToSchedule - numPendingTasks));
+    }
   }
 
   float getMinSourceVertexCompletedTaskFraction() {
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/api/TezRuntimeConfiguration.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/api/TezRuntimeConfiguration.java
index 85c53a509..6690f3e53 100644
--- a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/api/TezRuntimeConfiguration.java
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/api/TezRuntimeConfiguration.java
@@ -137,7 +137,7 @@ public class TezRuntimeConfiguration {
       TEZ_RUNTIME_PIPELINED_SORTER_MIN_BLOCK_SIZE_IN_MB = TEZ_RUNTIME_PREFIX +
       "pipelined.sorter.min-block.size.in.mb";
   public static final int
-      TEZ_RUNTIME_PIPELINED_SORTER_MIN_BLOCK_SIZE_IN_MB_DEFAULT = 2000;
+      TEZ_RUNTIME_PIPELINED_SORTER_MIN_BLOCK_SIZE_IN_MB_DEFAULT = 64;
 
   /**
    * Setting this to true would enable sorter
@@ -170,7 +170,7 @@ public class TezRuntimeConfiguration {
   @ConfigurationProperty(type = "integer")
   public static final String TEZ_RUNTIME_PIPELINED_SORTER_SORT_THREADS = TEZ_RUNTIME_PREFIX +
       "pipelined.sorter.sort.threads";
-  public static final int TEZ_RUNTIME_PIPELINED_SORTER_SORT_THREADS_DEFAULT = 2;
+  public static final int TEZ_RUNTIME_PIPELINED_SORTER_SORT_THREADS_DEFAULT = 10;
 
   /**
    * Integer value. Percentage of buffer to be filled before we spill to disk. Default value is 0,
@@ -639,6 +639,7 @@ public class TezRuntimeConfiguration {
     allowedPrefixes.add("io.");
     allowedPrefixes.add("file.");
     allowedPrefixes.add("fs.");
+    allowedPrefixes.add("tez.runtime.ess.");
 
     umnodifiableTezRuntimeKeySet = Collections.unmodifiableSet(tezRuntimeKeys);
     unmodifiableOtherKeySet = Collections.unmodifiableSet(otherKeys);
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/ValuesIterator.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/ValuesIterator.java
index 7add8c5ec..9185dddee 100644
--- a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/ValuesIterator.java
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/ValuesIterator.java
@@ -144,7 +144,9 @@ public class ValuesIterator<KEY,VALUE> {
             } catch (IOException ie) {
               throw new RuntimeException("problem advancing post rec#"+keyCtr, ie);
             }
-            inputValueCounter.increment(1);
+            if (inputValueCounter != null) {
+              inputValueCounter.increment(1);
+            }
             return value;
           }
 
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/comparator/ProxyComparator.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/comparator/ProxyComparator.java
index 5297f7cbd..a8f371d26 100644
--- a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/comparator/ProxyComparator.java
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/comparator/ProxyComparator.java
@@ -43,4 +43,5 @@ public interface ProxyComparator<KEY> extends RawComparator {
    */
   int getProxy(KEY key);
 
+  int getProxy(byte[] key, int keyLen);
 }
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/comparator/TezBytesComparator.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/comparator/TezBytesComparator.java
index 81df0cf7d..fa88916b0 100644
--- a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/comparator/TezBytesComparator.java
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/comparator/TezBytesComparator.java
@@ -60,4 +60,21 @@ public final class TezBytesComparator extends WritableComparator implements
     return prefix;
   }
 
+  @Override
+  public int getProxy(byte[] content, int len) {
+    int prefix = 0;
+    int b1 = 0, b2 = 0, b3 = 0;
+    switch (len) {
+      default:
+      case 3:
+        b3 = content[2] & 0xff;
+      case 2:
+        b2 = content[1] & 0xff;
+      case 1:
+        b1 = content[0] & 0xff;
+      case 0:
+    }
+    prefix = (b1 << 16) | (b2 << 8) | (b3);
+    return prefix;
+  }
 }
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/sort/impl/PipelinedSorterForRss.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/sort/impl/PipelinedSorterForRss.java
new file mode 100644
index 000000000..1ae03ef3e
--- /dev/null
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/sort/impl/PipelinedSorterForRss.java
@@ -0,0 +1,1172 @@
+  /**
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an "AS IS" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*/
+package org.apache.tez.runtime.library.common.sort.impl;
+
+  import com.google.common.annotations.VisibleForTesting;
+  import com.google.common.base.Preconditions;
+  import com.google.common.collect.Lists;
+  import com.google.common.collect.Maps;
+  import com.google.common.util.concurrent.ThreadFactoryBuilder;
+  import org.apache.hadoop.conf.Configuration;
+  import org.apache.hadoop.fs.FSDataOutputStream;
+  import org.apache.hadoop.fs.FileSystem;
+  import org.apache.hadoop.fs.Path;
+  import org.apache.hadoop.fs.permission.FsPermission;
+  import org.apache.hadoop.io.DataInputBuffer;
+  import org.apache.hadoop.io.RawComparator;
+  import org.apache.hadoop.io.compress.CompressionCodec;
+  import org.apache.hadoop.io.compress.DefaultCodec;
+  import org.apache.hadoop.util.*;
+  import org.apache.tez.common.CallableWithNdc;
+  import org.apache.tez.common.counters.TaskCounter;
+  import org.apache.tez.common.counters.TezCounter;
+  import org.apache.tez.common.io.NonSyncDataOutputStream;
+  import org.apache.tez.runtime.api.InputContext;
+  import org.apache.tez.runtime.library.api.IOInterruptedException;
+  import org.apache.tez.runtime.library.api.TezRuntimeConfiguration;
+  import org.apache.tez.runtime.library.common.ConfigUtils;
+  import org.apache.tez.runtime.library.common.comparator.ProxyComparator;
+  import org.apache.tez.runtime.library.common.sort.impl.IFile.Writer;
+  import org.apache.tez.runtime.library.common.sort.impl.TezMerger.DiskSegment;
+  import org.apache.tez.runtime.library.common.sort.impl.TezMerger.Segment;
+  import org.apache.tez.runtime.library.common.task.local.output.TezTaskOutput;
+  import org.apache.tez.runtime.library.common.task.local.output.TezTaskOutputFiles;
+  import org.apache.tez.runtime.library.utils.LocalProgress;
+  import org.slf4j.Logger;
+  import org.slf4j.LoggerFactory;
+
+  import java.io.IOException;
+  import java.io.OutputStream;
+  import java.nio.BufferOverflowException;
+  import java.nio.ByteBuffer;
+  import java.nio.ByteOrder;
+  import java.nio.IntBuffer;
+  import java.util.ArrayList;
+  import java.util.List;
+  import java.util.Map;
+  import java.util.PriorityQueue;
+  import java.util.concurrent.ExecutionException;
+  import java.util.concurrent.ExecutorService;
+  import java.util.concurrent.Executors;
+  import java.util.concurrent.Future;
+
+  import static org.apache.tez.runtime.library.common.sort.impl.TezSpillRecord.SPILL_FILE_PERMS;
+
+  @SuppressWarnings({"unchecked", "rawtypes"})
+  public class PipelinedSorterForRss {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PipelinedSorterForRss.class);
+
+    /**
+     * The size of each record in the index file for the map-outputs.
+     */
+    public static final int MAP_OUTPUT_INDEX_RECORD_LENGTH = 24;
+
+    private final static int APPROX_HEADER_LENGTH = 150;
+
+    private static final int KEYSTART = 0;         // key offset in acct
+    private static final int VALSTART = 1;         // val offset in acct
+    private static final int VALLEN = 2;           // val len in acct
+    private static final int NMETA = 3;            // num meta ints
+    private static final int METASIZE = NMETA * 4; // size in bytes
+
+    private final ProxyComparator hasher;
+    // SortSpans
+    private SortSpan span;
+
+    //total memory capacity allocated to sorter
+    private final long capacity;
+
+    //track buffer overflow recursively in all buffers
+    private int bufferOverflowRecursion;
+
+    // Merger
+    private SpanMerger merger;
+    private final ExecutorService sortmaster;
+
+    private final ArrayList<TezSpillRecord> indexCacheList =
+      new ArrayList<TezSpillRecord>();
+
+    private long currentAllocatableMemory;
+    //Maintain a list of ByteBuffers
+    @VisibleForTesting
+    List<ByteBuffer> buffers;
+    final int maxNumberOfBlocks;
+    private int bufferIndex = -1;
+    private final int MIN_BLOCK_SIZE;
+
+    private final Configuration conf;
+    private final RawComparator comparator;
+    private final InputContext inputContext;
+    private final long availableMemoryMb;
+    private final Class keyClass;
+    private final Class valClass;
+    private final IndexedSorter sorter;
+    private int numSpills;
+    private final TezTaskOutput shuffleReadOutput;
+    private final Map<Integer, Path> spillFilePaths = Maps.newHashMap();
+    private final Map<Integer, Path> spillFileIndexPaths = Maps.newHashMap();
+    private final FileSystem rfs;
+    private final boolean ifileReadAhead;
+    private final int ifileReadAheadLength;
+    private final int ifileBufferSize;
+    private final CompressionCodec codec;
+    private long inMemorySize;
+    private final TezCounter spilledRecordsCounter;
+
+    private final boolean cleanup;
+
+    private TezRawKeyValueIterator kvIter;
+
+    public TezRawKeyValueIterator getKvIter() {
+      return kvIter;
+    }
+
+    // TODO Set additional countesr - total bytes written, spills etc.
+    public PipelinedSorterForRss(InputContext inputContext, Configuration conf, int partitions,
+                                 long initialMemoryAvailable) throws IOException {
+
+      this.conf = conf;
+      comparator = ConfigUtils.getIntermediateInputKeyComparator(this.conf);
+      this.inputContext = inputContext;
+      this.availableMemoryMb = initialMemoryAvailable;
+
+      this.spilledRecordsCounter = inputContext.getCounters().findCounter(TaskCounter.SPILLED_RECORDS);
+      int minBlockSize = conf.getInt(TezRuntimeConfiguration
+              .TEZ_RUNTIME_PIPELINED_SORTER_MIN_BLOCK_SIZE_IN_MB,
+          TezRuntimeConfiguration
+              .TEZ_RUNTIME_PIPELINED_SORTER_MIN_BLOCK_SIZE_IN_MB_DEFAULT);
+      Preconditions.checkArgument(
+          (minBlockSize > 0 && minBlockSize < 2047),
+          TezRuntimeConfiguration
+              .TEZ_RUNTIME_PIPELINED_SORTER_MIN_BLOCK_SIZE_IN_MB
+              + "=" + minBlockSize + " should be a positive value between 0 and 2047");
+      MIN_BLOCK_SIZE = minBlockSize << 20;
+      LOG.info("MIN_BLOCK_SIZE " + MIN_BLOCK_SIZE);
+
+      //sanity checks
+      final long sortmb = this.availableMemoryMb;
+
+      // buffers and accounting
+      long maxMemLimit = sortmb << 20;
+
+      // k/v serialization
+      if(comparator instanceof ProxyComparator) {
+        hasher = (ProxyComparator)comparator;
+      } else {
+        hasher = null;
+      }
+
+      keyClass = ConfigUtils.getIntermediateOutputKeyClass(this.conf);
+      valClass = ConfigUtils.getIntermediateOutputValueClass(this.conf);
+
+      sorter = ReflectionUtils.newInstance(this.conf.getClass(
+          TezRuntimeConfiguration.TEZ_RUNTIME_INTERNAL_SORTER_CLASS, QuickSort.class,
+          IndexedSorter.class), this.conf);
+
+      cleanup = conf.getBoolean(TezRuntimeConfiguration.TEZ_RUNTIME_CLEANUP_FILES_ON_INTERRUPT,
+              TezRuntimeConfiguration.TEZ_RUNTIME_CLEANUP_FILES_ON_INTERRUPT_DEFAULT);
+
+      this.shuffleReadOutput = new TezTaskOutputFiles(conf,
+          inputContext.getUniqueIdentifier(), inputContext.getDagIdentifier());
+
+      rfs = (FileSystem.getLocal(this.conf)).getRaw();
+
+      this.ifileReadAhead = this.conf.getBoolean(
+          TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_READAHEAD,
+          TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_READAHEAD_DEFAULT);
+      if (this.ifileReadAhead) {
+        this.ifileReadAheadLength = conf.getInt(
+            TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_READAHEAD_BYTES,
+            TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_READAHEAD_BYTES_DEFAULT);
+      } else {
+        this.ifileReadAheadLength = 0;
+      }
+      this.ifileBufferSize = conf.getInt("io.file.buffer.size",
+          TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_BUFFER_SIZE_DEFAULT);
+      Class<? extends CompressionCodec> codecClass =
+          ConfigUtils.getIntermediateInputCompressorClass(this.conf, DefaultCodec.class);
+      codec = ReflectionUtils.newInstance(codecClass, this.conf);
+
+      long totalCapacityWithoutMeta = 0;
+      long availableMem = maxMemLimit;
+      int numBlocks = 0;
+      while(availableMem > 0) {
+        long size = Math.min(availableMem, computeBlockSize(availableMem, maxMemLimit));
+        int sizeWithoutMeta = (int) ((size) - (size % METASIZE));
+        totalCapacityWithoutMeta += sizeWithoutMeta;
+        availableMem -= size;
+        numBlocks++;
+      }
+      currentAllocatableMemory = maxMemLimit;
+      maxNumberOfBlocks = numBlocks;
+      capacity = totalCapacityWithoutMeta;
+
+      buffers = Lists.newArrayListWithCapacity(maxNumberOfBlocks);
+      allocateSpace(); //Allocate the first block
+
+      Preconditions.checkState(buffers.size() > 0, "Atleast one buffer needs to be present");
+
+      span = new SortSpan(buffers.get(bufferIndex), 1024 * 1024, 16, this.comparator);
+      merger = new SpanMerger(); // SpanIterators are comparable
+      final int sortThreads =
+              this.conf.getInt(
+                  TezRuntimeConfiguration.TEZ_RUNTIME_PIPELINED_SORTER_SORT_THREADS,
+                  TezRuntimeConfiguration.TEZ_RUNTIME_PIPELINED_SORTER_SORT_THREADS_DEFAULT);
+      LOG.info("num sort threads " + sortThreads);
+      sortmaster = Executors.newFixedThreadPool(sortThreads,
+          new ThreadFactoryBuilder().setDaemon(true)
+          .build());
+    }
+
+    ByteBuffer allocateSpace() {
+      if (currentAllocatableMemory <= 0) {
+        //No space available.
+        return null;
+      }
+
+      int size = computeBlockSize(currentAllocatableMemory, availableMemoryMb << 20);
+      currentAllocatableMemory -= size;
+      int sizeWithoutMeta = (size) - (size % METASIZE);
+      ByteBuffer space = ByteBuffer.allocate(sizeWithoutMeta);
+
+      buffers.add(space);
+      bufferIndex++;
+
+      Preconditions.checkState(buffers.size() <= maxNumberOfBlocks,
+          "Number of blocks " + buffers.size()
+              + " is exceeding  " + maxNumberOfBlocks);
+
+      LOG.info("Newly allocated block size=" + size
+          + ", index=" + bufferIndex
+          + ", Number of buffers=" + buffers.size()
+          + ", currentAllocatableMemory=" + currentAllocatableMemory
+          + ", currentBufferSize=" + space.capacity()
+          + ", total=" + (availableMemoryMb << 20));
+      return space;
+    }
+
+
+    @VisibleForTesting
+    int computeBlockSize(long availableMem, long maxAllocatedMemory) {
+      int maxBlockSize = 0;
+
+      //Honor MIN_BLOCK_SIZE
+      maxBlockSize = Math.max(MIN_BLOCK_SIZE, maxBlockSize);
+
+      if (availableMem < maxBlockSize) {
+        maxBlockSize = (int) availableMem;
+      }
+
+      int maxMem = (maxAllocatedMemory > Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int) maxAllocatedMemory;
+      if (maxBlockSize > maxMem) {
+        maxBlockSize = maxMem;
+      }
+
+      LOG.info("maxBlocksize " + maxBlockSize);
+      return maxBlockSize;
+    }
+
+    private int bitcount(int n) {
+      int bit = 0;
+      while(n!=0) {
+        bit++;
+        n >>= 1;
+      }
+      return bit;
+    }
+
+    public void sort() throws IOException {
+      SortSpan newSpan = span.next();
+
+      if(newSpan == null) {
+        //avoid sort/spill of empty span
+        // sort in the same thread, do not wait for the thread pool
+        merger.add(span.sort(sorter));
+        spill(true);
+        // clear buffers except one buffer
+        for (int i = 0; i < buffers.size() - 1; i++) {
+          buffers.get(0).clear();
+          buffers.remove(0);
+        }
+        //safe to reset bufferIndex to 0;
+        bufferIndex = 0;
+        int items = 1024*1024;
+        int perItem = 16;
+        if(span.length() != 0) {
+          items = span.length();
+          perItem = span.kvbuffer.limit()/items;
+          items = (int) ((span.capacity)/(METASIZE+perItem));
+          if(items > 1024*1024) {
+              // our goal is to have 1M splits and sort early
+              items = 1024*1024;
+          }
+        }
+        Preconditions.checkArgument(buffers.get(bufferIndex) != null, "block should not be empty");
+        //TODO: fix per item being passed.
+        span = new SortSpan((ByteBuffer)buffers.get(bufferIndex).clear(), (1024*1024),
+            perItem, ConfigUtils.getIntermediateOutputKeyComparator(this.conf));
+      } else {
+        // queue up the sort
+        SortTask task = new SortTask(span, sorter);
+        LOG.info("Submitting span={} for sort", span.toString());
+        Future<SpanIterator> future = sortmaster.submit(task);
+        inMemorySize += span.capacity;
+        merger.add(future);
+        span = newSpan;
+      }
+    }
+
+    public void write(int keyLen, int valLen, byte[] key, byte[] value)
+        throws IOException {
+      collect(keyLen, valLen, key, value);
+    }
+
+    /**
+     * Serialize the key, value to intermediate storage.
+     * When this method returns, kvindex must refer to sufficient unused
+     * storage to store one METADATA.
+     */
+    synchronized void collect(int keyLen, int valLen, byte[] key, byte[] value) throws IOException {
+      // TBD:FIX in TEZ-2574
+      if (span.kvmeta.remaining() < METASIZE) {
+        this.sort();
+        if (span.length() == 0) {
+          spillSingleRecord(key, value);
+          return;
+        }
+      }
+      int keystart = span.kvbuffer.position();
+      int valstart = -1;
+      int valend = -1;
+      try {
+        //keySerializer.serialize(key);
+        span.kvbuffer.put(key, 0, keyLen);
+        valstart = span.kvbuffer.position();
+        //valSerializer.serialize(value);
+        span.kvbuffer.put(value, 0, valLen);
+        valend = span.kvbuffer.position();
+      } catch(BufferOverflowException overflow) {
+        // restore limit
+        span.kvbuffer.position(keystart);
+        this.sort();
+        if (span.length() == 0 || bufferOverflowRecursion > buffers.size()) {
+          // spill the current key value pair
+          spillSingleRecord(key, value);
+          bufferOverflowRecursion = 0;
+          return;
+        }
+        bufferOverflowRecursion++;
+        // try again
+        this.collect(keyLen, valLen, key, value);
+        return;
+      }
+
+      if (bufferOverflowRecursion > 0) {
+        bufferOverflowRecursion--;
+      }
+
+      /* maintain order as in PARTITION, KEYSTART, VALSTART, VALLEN */
+      span.kvmeta.put(keystart);
+      span.kvmeta.put(valstart);
+      span.kvmeta.put(valend - valstart);
+    }
+
+    // it is guaranteed that when spillSingleRecord is called, there is
+    // no merger spans queued in executor.
+    // TODO giant record
+    private void spillSingleRecord(final Object key, final Object value) throws IOException {}
+    /*
+    private void spillSingleRecord(final Object key, final Object value) throws IOException {
+      final TezSpillRecord spillRec = new TezSpillRecord(partitions);
+      // getSpillFileForWrite with size -1 as the serialized size of KV pair is still unknown
+      final Path filename = shuffleReadOutput.getSpillFileForWrite(numSpills, -1);
+      Path indexFilename =
+          shuffleReadOutput.getSpillIndexFileForWrite(numSpills, partitions
+              * MAP_OUTPUT_INDEX_RECORD_LENGTH);
+      spillFilePaths.put(numSpills, filename);
+      FSDataOutputStream out = rfs.create(filename, true, 4096);
+      if (!SPILL_FILE_PERMS.equals(SPILL_FILE_PERMS.applyUMask(FsPermission.getUMask(conf)))) {
+        rfs.setPermission(filename, SPILL_FILE_PERMS);
+      }
+
+      try {
+        for (int i = 0; i < partitions; ++i) {
+          if (isThreadInterrupted()) {
+            return;
+          }
+          Writer writer = null;
+          try {
+            long segmentStart = out.getPos();
+            // we need not check for combiner since its a single record
+            if (i == partition) {
+              final long recordStart = out.getPos();
+              writer.append(key, value);
+            }
+            long rawLength = 0;
+            long partLength = 0;
+            if (writer != null) {
+              writer.close();
+              rawLength = writer.getRawLength();
+              partLength = writer.getCompressedLength();
+            }
+            // record offsets
+            final TezIndexRecord rec =
+                new TezIndexRecord(
+                    segmentStart, rawLength, partLength);
+            spillRec.putIndex(rec, i);
+            writer = null;
+          } finally {
+            if (null != writer) {
+              writer.close();
+            }
+          }
+        }
+
+        spillFileIndexPaths.put(numSpills, indexFilename);
+        spillRec.writeToFile(indexFilename, conf);
+        //TODO: honor cache limits
+        indexCacheList.add(spillRec);
+        spilledRecordsCounter.increment(1);
+        ++numSpills;
+      } finally {
+          out.close();
+          inMemorySize = 0;
+      }
+    }
+     */
+
+    public boolean spill(boolean ignoreEmptySpills) throws IOException {
+      LOG.info("inside spill");
+      FSDataOutputStream out = null;
+      try {
+        try {
+          boolean ret = merger.ready();
+          // if merger returned false and ignore merge is true,
+          // then return directly without spilling
+          if (!ret && ignoreEmptySpills){
+            return false;
+          }
+        } catch (InterruptedException e) {
+          Thread.currentThread().interrupt();
+        }
+
+        // create spill file
+        final long size = capacity + APPROX_HEADER_LENGTH;
+        final TezSpillRecord spillRec = new TezSpillRecord(1);
+        final Path filename =
+          shuffleReadOutput.getSpillFileForWrite(numSpills, size);
+        LOG.info("spill file path " + filename);
+        spillFilePaths.put(numSpills, filename);
+        out = rfs.create(filename, true, 4096);
+        if (!SPILL_FILE_PERMS.equals(SPILL_FILE_PERMS.applyUMask(FsPermission.getUMask(conf)))) {
+          rfs.setPermission(filename, SPILL_FILE_PERMS);
+        }
+        if (isThreadInterrupted()) {
+          return false;
+        }
+        TezRawKeyValueIterator kvIter = merger;
+        //write merged output to disk
+        long segmentStart = out.getPos();
+        Writer writer = null;
+        boolean hasNext = kvIter.hasNext();
+        if (hasNext) {
+          writer = new Writer(conf, out, keyClass, valClass, codec,
+              spilledRecordsCounter, null, merger.needsRLE());
+        }
+        while (kvIter.next()) {
+          writer.append(kvIter.getKey(), kvIter.getValue());
+        }
+        long rawLength = 0;
+        long partLength = 0;
+        //close
+        if (writer != null) {
+          writer.close();
+          rawLength = writer.getRawLength();
+          partLength = writer.getCompressedLength();
+        }
+        // record offsets
+        final TezIndexRecord rec =
+            new TezIndexRecord(segmentStart, rawLength, partLength);
+        spillRec.putIndex(rec, 0);
+
+        Path indexFilename =
+          shuffleReadOutput.getSpillIndexFileForWrite(numSpills, MAP_OUTPUT_INDEX_RECORD_LENGTH);
+        spillFileIndexPaths.put(numSpills, indexFilename);
+        spillRec.writeToFile(indexFilename, conf);
+        //TODO: honor cache limits
+        indexCacheList.add(spillRec);
+        ++numSpills;
+        return true;
+      } finally {
+        if (out != null) {
+          out.close();
+        }
+        inMemorySize = 0;
+      }
+    }
+
+    private boolean isThreadInterrupted() throws IOException {
+      if (Thread.currentThread().isInterrupted()) {
+        if (cleanup) {
+          cleanup();
+        }
+        sortmaster.shutdownNow();
+        return true;
+      }
+      return false;
+    }
+
+    private synchronized void cleanup() throws IOException {
+      if (!cleanup) {
+        return;
+      }
+      cleanup(spillFilePaths);
+      cleanup(spillFileIndexPaths);
+    }
+
+    private synchronized void cleanup(Map<Integer, Path> spillMap) {
+      for (Map.Entry<Integer, Path> entry : spillMap.entrySet()) {
+        Path path = entry.getValue();
+        if (path != null) {
+          try {
+            LOG.info("Deleting " + path);
+            rfs.delete(path, true);
+          } catch (IOException ioe) {
+            LOG.warn("Error in deleting " + path);
+          }
+        }
+      }
+    }
+
+    public void flush() throws IOException {
+      /**
+       * Possible that the thread got interrupted when flush was happening or when the flush was
+       * never invoked. As a part of cleanup activity in TezTaskRunner, it would invoke close()
+       * on all I/O. At that time, this is safe to cleanup
+       */
+      if (isThreadInterrupted()) {
+        return;
+      }
+
+      try {
+        span.end();
+        merger.add(span.sort(sorter));
+        // force a spill in flush()
+        // case 1: we want to force because of following scenarios:
+        // we have no keys written, and flush got called
+        // we want atleast one spill(be it empty)
+        // case 2: in pipeline shuffle case, we have no way of
+        // knowing the last key being written until flush is called
+        // so for flush()->spill() we want to force spill so that
+        // we can send pipeline shuffle event with last event true.
+//        spill(false);
+        merger.ready();
+        TezRawKeyValueIterator lastIter = merger;
+        Segment lastSegment = new TezMerger.IteratorSegment(lastIter, inMemorySize + span.capacity);
+        sortmaster.shutdown();
+
+        //safe to clean up
+//        buffers.clear();
+//        buffers = null;
+//        span = null;
+//        LOG.info("after flush, buffers cleared");
+
+        if(!lastIter.hasNext() && indexCacheList.isEmpty()) {
+          LOG.info("no data, just return");
+          return;
+        }
+
+        int parts = 0;
+        //create the segments to be merged
+        List<Segment> segmentList =
+            new ArrayList<Segment>(numSpills);
+        for (int i = 0; i < numSpills; i++) {
+          Path spillFilename = spillFilePaths.get(i);
+          TezIndexRecord indexRecord = indexCacheList.get(i).getIndex(parts);
+          if (indexRecord.hasData()) {
+            DiskSegment s =
+                new DiskSegment(rfs, spillFilename, indexRecord.getStartOffset(),
+                    indexRecord.getPartLength(), codec, ifileReadAhead,
+                    ifileReadAheadLength, ifileBufferSize, true);
+            segmentList.add(s);
+          }
+        }
+        segmentList.add(lastSegment);
+
+        int mergeFactor =
+            this.conf.getInt(TezRuntimeConfiguration.TEZ_RUNTIME_IO_SORT_FACTOR,
+                TezRuntimeConfiguration.TEZ_RUNTIME_IO_SORT_FACTOR_DEFAULT);
+        // sort the segments only if there are intermediate merges
+//        boolean sortSegments = segmentList.size() > mergeFactor;
+        boolean sortSegments = false;
+        //merge
+        kvIter = TezMerger.merge(conf, rfs,
+            keyClass, valClass, codec,
+            segmentList, mergeFactor,
+            new Path(inputContext.getUniqueIdentifier()),
+            ConfigUtils.getIntermediateOutputKeyComparator(conf),
+            () -> {}, sortSegments, true,
+            null, null, null,
+            null, merger.needsRLE()); // Not using any Progress in TezMerger. Should just work.
+
+        // TODO delete intermediate files
+      } catch(InterruptedException ie) {
+        if (cleanup) {
+          cleanup();
+        }
+        Thread.currentThread().interrupt();
+        throw new IOInterruptedException("Interrupted while closing Output", ie);
+      }
+    }
+
+
+    public void close() {
+      if (buffers != null) {
+        buffers.clear();
+        buffers = null;
+      }
+      merger = null;
+      span = null;
+    }
+
+    private static class BufferStreamWrapper extends OutputStream
+    {
+      private final ByteBuffer out;
+      public BufferStreamWrapper(ByteBuffer out) {
+        this.out = out;
+      }
+
+      @Override
+      public void write(int b) throws IOException { out.put((byte)b); }
+      @Override
+      public void write(byte[] b) throws IOException { out.put(b); }
+      @Override
+      public void write(byte[] b, int off, int len) throws IOException { out.put(b, off, len); }
+    }
+
+    private static final class InputByteBuffer extends DataInputBuffer {
+      private byte[] buffer = new byte[256];
+      private ByteBuffer wrapped = ByteBuffer.wrap(buffer);
+      private void resize(int length) {
+        if(length > buffer.length || (buffer.length > 10 * (1+length))) {
+          // scale down as well as scale up across values
+          buffer = new byte[length];
+          wrapped = ByteBuffer.wrap(buffer);
+        }
+        wrapped.limit(length);
+      }
+
+      // shallow copy
+      public void reset(DataInputBuffer clone) {
+        byte[] data = clone.getData();
+        int start = clone.getPosition();
+        int length = clone.getLength() - start;
+        super.reset(data, start, length);
+      }
+
+      // deep copy
+      @SuppressWarnings("unused")
+      public void copy(DataInputBuffer clone) {
+        byte[] data = clone.getData();
+        int start = clone.getPosition();
+        int length = clone.getLength() - start;
+        resize(length);
+        System.arraycopy(data, start, buffer, 0, length);
+        super.reset(buffer, 0, length);
+      }
+    }
+
+    private final class SortSpan implements IndexedSortable {
+      final IntBuffer kvmeta;
+      final byte[] rawkvmeta;
+      final int kvmetabase;
+      final ByteBuffer kvbuffer;
+      final NonSyncDataOutputStream out;
+      final RawComparator comparator;
+      final byte[] imeta = new byte[METASIZE];
+
+      private int index = 0;
+      private long eq = 0;
+      private boolean reinit = false;
+      private int capacity;
+
+
+      public SortSpan(ByteBuffer source, int maxItems, int perItem, RawComparator comparator) {
+        capacity = source.remaining();
+        int metasize = METASIZE*maxItems;
+        int dataSize = maxItems * perItem;
+        if(capacity < (metasize+dataSize)) {
+          // try to allocate less meta space, because we have sample data
+          metasize = METASIZE*(capacity/(perItem+METASIZE));
+        }
+        ByteBuffer reserved = source.duplicate();
+        reserved.mark();
+        reserved.position(metasize);
+        kvbuffer = reserved.slice();
+        reserved.flip();
+        reserved.limit(metasize);
+        ByteBuffer kvmetabuffer = reserved.slice();
+        rawkvmeta = kvmetabuffer.array();
+        kvmetabase = kvmetabuffer.arrayOffset();
+        kvmeta = kvmetabuffer
+                  .order(ByteOrder.nativeOrder())
+                 .asIntBuffer();
+        out = new NonSyncDataOutputStream(
+                new BufferStreamWrapper(kvbuffer));
+        this.comparator = comparator;
+      }
+
+      public SpanIterator sort(IndexedSorter sorter) {
+        if(length() > 1) {
+          sorter.sort(this, 0, length(), () -> {});
+        }
+        return new SpanIterator(this);
+      }
+
+      int offsetFor(int i) {
+        return (i * NMETA);
+      }
+
+      public void swap(final int mi, final int mj) {
+        final int kvi = offsetFor(mi);
+        final int kvj = offsetFor(mj);
+
+        final int kvioff = kvmetabase + (kvi << 2);
+        final int kvjoff = kvmetabase + (kvj << 2);
+        System.arraycopy(rawkvmeta, kvioff, imeta, 0, METASIZE);
+        System.arraycopy(rawkvmeta, kvjoff, rawkvmeta, kvioff, METASIZE);
+        System.arraycopy(imeta, 0, rawkvmeta, kvjoff, METASIZE);
+      }
+
+      protected int compareKeys(final int kvi, final int kvj) {
+        final int istart = kvmeta.get(kvi + KEYSTART);
+        final int jstart = kvmeta.get(kvj + KEYSTART);
+        final int ilen   = kvmeta.get(kvi + VALSTART) - istart;
+        final int jlen   = kvmeta.get(kvj + VALSTART) - jstart;
+
+        if (ilen == 0 || jlen == 0) {
+          if (ilen == jlen) {
+            eq++;
+          }
+          return ilen - jlen;
+        }
+
+        final byte[] buf = kvbuffer.array();
+        final int off = kvbuffer.arrayOffset();
+
+        // sort by key
+        final int cmp = comparator.compare(buf, off + istart, ilen, buf, off + jstart, jlen);
+        if(cmp == 0) eq++;
+        return cmp;
+      }
+
+
+      public int compare(final int mi, final int mj) {
+        final int kvi = offsetFor(mi);
+        final int kvj = offsetFor(mj);
+        final int jstart = kvmeta.get(kvj + KEYSTART);
+        final int jlen   = kvmeta.get(kvj + VALSTART) - jstart;
+        final int istart = kvmeta.get(kvi + KEYSTART);
+        final int ilen   = kvmeta.get(kvi + VALSTART) - istart;
+
+        if (ilen == 0 || jlen == 0) {
+          if (ilen == jlen) {
+            eq++;
+          }
+          return ilen - jlen;
+        }
+
+        final byte[] buf = kvbuffer.array();
+        final int off = kvbuffer.arrayOffset();
+
+        // sort by key
+        final int cmp = comparator.compare(buf, off + istart, ilen, buf, off + jstart, jlen);
+        if(cmp == 0) eq++;
+        return cmp;
+      }
+
+      public SortSpan next() {
+        ByteBuffer remaining = end();
+        if(remaining != null) {
+          SortSpan newSpan = null;
+          int items = length();
+          int perItem = kvbuffer.position()/items;
+          if (reinit) { //next mem block
+            //quite possible that the previous span had a length of 1. It is better to reinit here for new span.
+            items = 1024*1024;
+            perItem = 16;
+          }
+          final RawComparator newComparator = ConfigUtils.getIntermediateOutputKeyComparator(conf);
+          if (this.comparator == newComparator) {
+            LOG.warn("Same comparator used. comparator={}, newComparator={},"
+                    + " hashCode: comparator={}, newComparator={}",
+                this.comparator, newComparator,
+                System.identityHashCode(this.comparator),
+                System.identityHashCode(newComparator));
+          }
+          newSpan = new SortSpan(remaining, items, perItem, newComparator);
+          newSpan.index = index+1;
+          return newSpan;
+        }
+        return null;
+      }
+
+      public int length() {
+        return kvmeta.limit()/NMETA;
+      }
+
+      public ByteBuffer end() {
+        ByteBuffer remaining = kvbuffer.duplicate();
+        remaining.position(kvbuffer.position());
+        remaining = remaining.slice();
+        kvbuffer.limit(kvbuffer.position());
+        kvmeta.limit(kvmeta.position());
+        int items = length();
+        if(items == 0) {
+          return null;
+        }
+        int perItem = kvbuffer.position()/items;
+        if(remaining.remaining() < METASIZE+perItem) {
+          //Check if we can get the next Buffer from the main buffer list
+          LOG.info("allocate space in end");
+          ByteBuffer space = allocateSpace();
+          if (space != null) {
+            reinit = true;
+            return space;
+          }
+          return null;
+        }
+        return remaining;
+      }
+
+      public int compareInternal(final DataInputBuffer needle, final int index) {
+        final int keystart;
+        final int valstart;
+        keystart = kvmeta.get(this.offsetFor(index) + KEYSTART);
+        valstart = kvmeta.get(this.offsetFor(index) + VALSTART);
+        final byte[] buf = kvbuffer.array();
+        final int off = kvbuffer.arrayOffset();
+        int cmp = comparator.compare(buf,
+            keystart + off , (valstart - keystart),
+            needle.getData(),
+            needle.getPosition(), (needle.getLength() - needle.getPosition()));
+        return cmp;
+      }
+
+      public long getEq() {
+        return eq;
+      }
+
+      @Override
+      public String toString() {
+          return String.format("Span[%d,%d]", NMETA*kvmeta.capacity(), kvbuffer.limit());
+      }
+    }
+
+    private static class SpanIterator implements TezRawKeyValueIterator, Comparable<SpanIterator> {
+      private int kvindex = -1;
+      private final int maxindex;
+      private final IntBuffer kvmeta;
+      private final ByteBuffer kvbuffer;
+      private SortSpan span;
+      private final InputByteBuffer key = new InputByteBuffer();
+      private final InputByteBuffer value = new InputByteBuffer();
+      private final Progress progress = new LocalProgress();
+
+      private static final int minrun = (1 << 4);
+
+      public SpanIterator(SortSpan span) {
+        this.kvmeta = span.kvmeta;
+        this.kvbuffer = span.kvbuffer;
+        this.span = span;
+        this.maxindex = (kvmeta.limit()/NMETA) - 1;
+      }
+
+      public DataInputBuffer getKey()  {
+        final int keystart = kvmeta.get(span.offsetFor(kvindex) + KEYSTART);
+        final int valstart = kvmeta.get(span.offsetFor(kvindex) + VALSTART);
+        final byte[] buf = kvbuffer.array();
+        final int off = kvbuffer.arrayOffset();
+        key.reset(buf, off + keystart, valstart - keystart);
+        return key;
+      }
+
+      public DataInputBuffer getValue() {
+        final int valstart = kvmeta.get(span.offsetFor(kvindex) + VALSTART);
+        final int vallen = kvmeta.get(span.offsetFor(kvindex) + VALLEN);
+        final byte[] buf = kvbuffer.array();
+        final int off = kvbuffer.arrayOffset();
+        value.reset(buf, off + valstart, vallen);
+        return value;
+      }
+
+      public boolean next() {
+        // caveat: since we use this as a comparable in the merger
+        if(kvindex == maxindex) return false;
+        kvindex += 1;
+        if(kvindex % 100 == 0) {
+          progress.set(1 - ((maxindex - kvindex) / (float) maxindex));
+        }
+        return true;
+      }
+
+      @Override
+      public boolean hasNext() {
+        return (kvindex == maxindex);
+      }
+
+      public void close() {
+        span = null;
+      }
+
+      public Progress getProgress() {
+        return progress;
+      }
+
+      @Override
+      public boolean isSameKey() throws IOException {
+        return false;
+      }
+
+      @SuppressWarnings("unused")
+      public int size() {
+        return (maxindex - kvindex);
+      }
+
+      public int compareTo(SpanIterator other) {
+        return span.compareInternal(other.getKey(), kvindex);
+      }
+
+      @Override
+      public String toString() {
+        return String.format("SpanIterator<%d:%d> (span=%s)", kvindex, maxindex, span.toString());
+      }
+
+      /**
+       * bisect returns the next insertion point for a given raw key, skipping keys
+       * which are <= needle using a binary search instead of a linear comparison.
+       * This is massively efficient when long strings of identical keys occur.
+       * @param needle
+       * @return
+       */
+      int bisect(DataInputBuffer needle) {
+        int start = kvindex;
+        int end = maxindex-1;
+        int mid = start;
+        int cmp = 0;
+
+        if(end - start < minrun) {
+          return 0;
+        }
+
+        if(span.compareInternal(needle, start) > 0) {
+          return kvindex;
+        }
+
+        // bail out early if we haven't got a min run
+        if(span.compareInternal(needle, start+minrun) > 0) {
+          return 0;
+        }
+
+        if(span.compareInternal(needle, end) < 0) {
+          return end - kvindex;
+        }
+
+        boolean found = false;
+
+        // we sort 100k items, the max it can do is 20 loops, but break early
+        for(int i = 0; start < end && i < 16; i++) {
+          mid = start + (end - start)/2;
+          cmp = span.compareInternal(needle, mid);
+          if(cmp == 0) {
+            start = mid;
+            found = true;
+          } else if(cmp < 0) {
+            start = mid;
+            found = true;
+          }
+          if(cmp > 0) {
+            end = mid;
+          }
+        }
+
+        if(found) {
+          return start - kvindex;
+        }
+        return 0;
+      }
+    }
+
+    private static class SortTask extends CallableWithNdc<SpanIterator> {
+      private final SortSpan sortable;
+      private final IndexedSorter sorter;
+
+      public SortTask(SortSpan sortable, IndexedSorter sorter) {
+          this.sortable = sortable;
+          this.sorter = sorter;
+      }
+
+      @Override
+      protected SpanIterator callInternal() {
+        return sortable.sort(sorter);
+      }
+    }
+
+    private static class SpanHeap extends PriorityQueue<SpanIterator> {
+      private static final long serialVersionUID = 1L;
+
+      public SpanHeap() {
+        super(256);
+      }
+      /**
+       * {@link PriorityQueue}.poll() by a different name
+       * @return
+       */
+      public SpanIterator pop() {
+        return this.poll();
+      }
+    }
+
+    private final class SpanMerger implements TezRawKeyValueIterator {
+      InputByteBuffer key = new InputByteBuffer();
+      InputByteBuffer value = new InputByteBuffer();
+      int partition;
+
+      private ArrayList< Future<SpanIterator>> futures = new ArrayList< Future<SpanIterator>>();
+
+      private SpanHeap heap = new SpanHeap();
+
+      private int gallop = 0;
+      private SpanIterator horse;
+      private long total = 0;
+      private long eq = 0;
+
+      public SpanMerger() {
+      }
+
+      public final void add(SpanIterator iter) {
+        if(iter.next()) {
+          heap.add(iter);
+        }
+      }
+
+      public final void add(Future<SpanIterator> iter) {
+        this.futures.add(iter);
+      }
+
+      public final boolean ready() throws IOException, InterruptedException {
+        int numSpanItr = futures.size();
+        try {
+          SpanIterator iter = null;
+          while(this.futures.size() > 0) {
+            Future<SpanIterator> futureIter = this.futures.remove(0);
+            iter = futureIter.get();
+            this.add(iter);
+          }
+
+          StringBuilder sb = new StringBuilder();
+          if (heap.size() == 0) {
+            return false;
+          }
+          for(SpanIterator sp: heap) {
+              sb.append(sp.toString());
+              sb.append(",");
+              total += sp.span.length();
+              eq += sp.span.getEq();
+          }
+          return true;
+        } catch(ExecutionException e) {
+          LOG.error("Heap size={}, total={}, eq={}, partition={}, gallop={}, totalItr={},"
+                  + " futures.size={}",
+              heap.size(), total, eq, partition, gallop, numSpanItr, futures.size(), e);
+          throw new IOException(e);
+        }
+      }
+
+      private SpanIterator pop() {
+        if(gallop > 0) {
+          gallop--;
+          return horse;
+        }
+        SpanIterator current = heap.pop();
+        SpanIterator next = heap.peek();
+        if(next != null && current != null &&
+          ((Object)horse) == ((Object)current)) {
+          // TODO: a better threshold check than 1 key repeating
+          gallop = current.bisect(next.getKey()) - 1;
+        }
+        horse = current;
+        return current;
+      }
+
+      public boolean needsRLE() {
+        return (eq > 0.1 * total);
+      }
+
+      @SuppressWarnings("unused")
+      private SpanIterator peek() {
+        if (gallop > 0) {
+          return horse;
+        }
+        return heap.peek();
+      }
+
+      public final boolean next() {
+        SpanIterator current = pop();
+
+        if(current != null) {
+          key.reset(current.getKey());
+          value.reset(current.getValue());
+          if(gallop <= 0) {
+            // since all keys and values are references to the kvbuffer, no more deep copies
+            this.add(current);
+          } else {
+            // galloping, no deep copies required anyway
+            current.next();
+          }
+          return true;
+        }
+        return false;
+      }
+
+      @Override
+      public boolean hasNext() {
+        return peek() != null;
+      }
+
+      public DataInputBuffer getKey() { return key; }
+      public DataInputBuffer getValue() { return value; }
+      public int getPartition() { return partition; }
+
+      public void close() throws IOException {
+        this.horse = null;
+        this.heap = null;
+      }
+
+      public Progress getProgress() {
+        // TODO
+        return new Progress();
+      }
+
+      @Override
+      public boolean isSameKey() throws IOException {
+        return false;
+      }
+    }
+  }
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/sort/impl/TezMerger.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/sort/impl/TezMerger.java
index 0e18eadae..e2270866c 100644
--- a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/sort/impl/TezMerger.java
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/sort/impl/TezMerger.java
@@ -339,6 +339,92 @@ public class TezMerger {
     }
   }
 
+  public static class IteratorSegment extends Segment {
+    private TezRawKeyValueIterator iter;
+    private long length;
+    private long bytesRead;
+    public IteratorSegment(TezRawKeyValueIterator iter, long length) {
+      super(null, null);
+      this.iter = iter;
+      this.length = length;
+    }
+
+    @Override
+    KeyValueBuffer getKey() {
+      try {
+        DataInputBuffer buffer = iter.getKey();
+        key.reset(buffer.getData(), buffer.getPosition(), buffer.getLength() - buffer.getPosition());
+        bytesRead += key.getLength();
+        return key;
+      } catch (Exception e) {
+        LOG.error("Error in getKey", e);
+        return null;
+      }
+    }
+
+    @Override
+    DataInputBuffer getValue(DataInputBuffer value) throws IOException {
+      DataInputBuffer val = iter.getValue();
+      bytesRead += val.getLength();
+      value.reset(val.getData(), val.getPosition(), val.getLength() - val.getPosition());
+      return value;
+    }
+
+    @Override
+    public long getLength() {
+      return length;
+    }
+
+    @Override
+    KeyState readRawKey(DataInputBuffer nextKey) throws IOException {
+      KeyState keyState;
+      if (iter.next()) {
+        if (iter.isSameKey()) {
+          keyState = KeyState.SAME_KEY;
+        } else {
+          keyState = KeyState.NEW_KEY;
+        }
+      } else {
+        keyState = KeyState.NO_KEY;
+      }
+      return keyState;
+    }
+
+    boolean nextRawKey(DataInputBuffer nextKey) throws IOException {
+      return iter.next();
+    }
+
+    void nextRawValue(DataInputBuffer value) throws IOException {
+    }
+
+    void closeReader() throws IOException {
+    }
+
+    void close() throws IOException {
+      iter.close();
+      iter = null;
+    }
+
+    public long getPosition() throws IOException {
+      return bytesRead;
+    }
+
+    // This method is used by BackupStore to extract the
+    // absolute position after a reset
+    long getActualPosition() throws IOException {
+      return bytesRead;
+    }
+
+    Reader getReader() {
+      return null;
+    }
+
+    // This method is used by BackupStore to reinitialize the
+    // reader to start reading from a different segment offset
+    void reinitReader(int offset) throws IOException {
+    }
+  }
+
   @InterfaceAudience.Private
   @InterfaceStability.Unstable
   public static class DiskSegment extends Segment {
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/sort/impl/dflt/DefaultSorterForRss.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/sort/impl/dflt/DefaultSorterForRss.java
new file mode 100644
index 000000000..e27d863ca
--- /dev/null
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/sort/impl/dflt/DefaultSorterForRss.java
@@ -0,0 +1,1185 @@
+package org.apache.tez.runtime.library.common.sort.impl.dflt;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Maps;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocalFileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.DefaultCodec;
+import org.apache.hadoop.io.serializer.SerializationFactory;
+import org.apache.hadoop.io.serializer.Serializer;
+import org.apache.hadoop.util.*;
+import org.apache.tez.common.io.NonSyncDataOutputStream;
+import org.apache.tez.runtime.api.InputContext;
+import org.apache.tez.runtime.library.api.IOInterruptedException;
+import org.apache.tez.runtime.library.api.Partitioner;
+import org.apache.tez.runtime.library.api.TezRuntimeConfiguration;
+import org.apache.tez.runtime.library.common.ConfigUtils;
+import org.apache.tez.runtime.library.common.TezRuntimeUtils;
+import org.apache.tez.runtime.library.common.sort.impl.*;
+import org.apache.tez.runtime.library.common.task.local.output.TezTaskOutput;
+import org.apache.tez.runtime.library.common.task.local.output.TezTaskOutputFiles;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.io.OutputStream;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.IntBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.ReentrantLock;
+
+import static org.apache.tez.runtime.library.common.sort.impl.TezSpillRecord.SPILL_FILE_PERMS;
+
+/**
+ * Reference from {@link org.apache.tez.runtime.library.common.sort.impl.dflt.DefaultSorter}
+ */
+public class DefaultSorterForRss implements IndexedSortable {
+    private static final Logger LOG = LoggerFactory.getLogger(DefaultSorterForRss.class);
+
+    /**
+     * The size of each record in the index file for the outputs.
+     */
+    public static final int MAP_OUTPUT_INDEX_RECORD_LENGTH = 24;
+
+    private final static int APPROX_HEADER_LENGTH = 150;
+
+    private final InputContext inputContext;
+
+    private final TezTaskOutput shuffleReadOutput;
+
+    private TezRawKeyValueIterator kvIter;
+
+    private final FileSystem rfs;
+
+    private AtomicLong inputByteCounter = new AtomicLong(0);
+    private AtomicLong inputRecordCounter = new AtomicLong(0);
+    private int numSpills;
+
+    private final Map<Integer, Path> spillFilePaths = Maps.newHashMap();
+    private final Map<Integer, Path> spillFileIndexPaths = Maps.newHashMap();
+
+    private final IndexedSorter sorter;
+
+    private final boolean ifileReadAhead;
+    private final int ifileReadAheadLength;
+    private final int ifileBufferSize;
+
+    // k/v accounting
+    private IntBuffer kvmeta; // metadata overlay on backing store
+    int kvstart;            // marks origin of spill metadata
+    int kvend;              // marks end of spill metadata
+    int kvindex;            // marks end of fully serialized records
+
+    int equator;            // marks origin of meta/serialization
+    int bufstart;           // marks beginning of spill
+    int bufend;             // marks beginning of collectable
+    int bufmark;            // marks end of record
+    int bufindex;           // marks end of collected
+    int bufvoid;            // marks the point where we should stop
+
+    private byte[] kvbuffer;        // main output buffer
+    private final byte[] b0 = new byte[0];
+
+    protected static final int VALSTART = 0;         // val offset in acct
+    protected static final int KEYSTART = 1;         // key offset in acct
+    protected static final int PARTITION = 2;        // partition offset in acct
+    protected static final int VALLEN = 3;           // length of value
+    protected static final int NMETA = 4;            // num meta ints
+    protected static final int METASIZE = NMETA * 4; // size in bytes
+
+    // spill accounting
+    final int maxRec;
+    final int softLimit;
+    boolean spillInProgress;
+    int bufferRemaining;
+    volatile Throwable sortSpillException = null;
+
+    final int minSpillsForCombine;
+    final ReentrantLock spillLock = new ReentrantLock();
+    final Condition spillDone = spillLock.newCondition();
+    final Condition spillReady = spillLock.newCondition();
+    final DefaultSorterForRss.BlockingBuffer bb = new DefaultSorterForRss.BlockingBuffer();
+    volatile boolean spillThreadRunning = false;
+    final DefaultSorterForRss.SpillThread spillThread = new DefaultSorterForRss.SpillThread();
+
+    final ArrayList<TezSpillRecord> indexCacheList =
+        new ArrayList<TezSpillRecord>();
+    private final int indexCacheMemoryLimit;
+    private int totalIndexCacheMemory;
+
+    private long totalKeys = 0;
+    private long sameKey = 0;
+
+    public static final int MAX_IO_SORT_MB = 1800;
+
+    private final Configuration conf;
+    private final long availableMemoryMb;
+
+    private final Class keyClass;
+    private final Class valClass;
+    protected final RawComparator comparator;
+    private final SerializationFactory serializationFactory;
+    private final Serializer keySerializer;
+    private final Serializer valSerializer;
+    // Compression for map-outputs
+    private final CompressionCodec codec;
+
+    private final boolean cleanup;
+
+    private final Partitioner partitioner;
+
+    private final int partitions = 1; // TODO: if used for output, it can not be 1
+
+    public DefaultSorterForRss(InputContext inputContext, Configuration conf, long initialMemoryAvailable) throws IOException {
+        this.inputContext = inputContext;
+        this.shuffleReadOutput = new TezTaskOutputFiles(conf,
+            inputContext.getUniqueIdentifier(), inputContext.getDagIdentifier());
+        this.conf = conf;
+        this.partitioner = TezRuntimeUtils.instantiatePartitioner(this.conf);
+        //Let the overflow checks happen in appropriate sorter impls
+        this.availableMemoryMb = initialMemoryAvailable;
+        // sanity checks
+        final float spillper = this.conf.getFloat(
+            TezRuntimeConfiguration.TEZ_RUNTIME_SORT_SPILL_PERCENT,
+            TezRuntimeConfiguration.TEZ_RUNTIME_SORT_SPILL_PERCENT_DEFAULT);
+        final int sortmb = computeSortBufferSize((int) availableMemoryMb,"");
+
+        Preconditions.checkArgument(spillper <= (float) 1.0 && spillper > (float) 0.0,
+            TezRuntimeConfiguration.TEZ_RUNTIME_SORT_SPILL_PERCENT
+                + " should be greater than 0 and less than or equal to 1");
+
+        rfs = ((LocalFileSystem)FileSystem.getLocal(this.conf)).getRaw();
+
+        Class<? extends CompressionCodec> codecClass =
+            ConfigUtils.getIntermediateInputCompressorClass(this.conf, DefaultCodec.class);
+        codec = ReflectionUtils.newInstance(codecClass, this.conf);
+
+        // sorter
+        sorter = ReflectionUtils.newInstance(this.conf.getClass(
+            TezRuntimeConfiguration.TEZ_RUNTIME_INTERNAL_SORTER_CLASS, QuickSort.class,
+            IndexedSorter.class), this.conf);
+
+        cleanup = conf.getBoolean(TezRuntimeConfiguration.TEZ_RUNTIME_CLEANUP_FILES_ON_INTERRUPT,
+                TezRuntimeConfiguration.TEZ_RUNTIME_CLEANUP_FILES_ON_INTERRUPT_DEFAULT);
+
+        kvIter = new EmptyIterator();
+
+        indexCacheMemoryLimit = this.conf.getInt(TezRuntimeConfiguration.TEZ_RUNTIME_INDEX_CACHE_MEMORY_LIMIT_BYTES,
+            TezRuntimeConfiguration.TEZ_RUNTIME_INDEX_CACHE_MEMORY_LIMIT_BYTES_DEFAULT);
+
+        comparator = ConfigUtils.getIntermediateInputKeyComparator(this.conf);
+
+        this.ifileReadAhead = this.conf.getBoolean(
+            TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_READAHEAD,
+            TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_READAHEAD_DEFAULT);
+        if (this.ifileReadAhead) {
+            this.ifileReadAheadLength = conf.getInt(
+                TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_READAHEAD_BYTES,
+                TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_READAHEAD_BYTES_DEFAULT);
+        } else {
+            this.ifileReadAheadLength = 0;
+        }
+        this.ifileBufferSize = conf.getInt("io.file.buffer.size",
+            TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_BUFFER_SIZE_DEFAULT);
+
+        // buffers and accounting
+        int maxMemUsage = sortmb << 20;
+        maxMemUsage -= maxMemUsage % METASIZE;
+        kvbuffer = new byte[maxMemUsage];
+        bufvoid = kvbuffer.length;
+        kvmeta = ByteBuffer.wrap(kvbuffer)
+            .order(ByteOrder.nativeOrder())
+            .asIntBuffer();
+        setEquator(0);
+        bufstart = bufend = bufindex = equator;
+        kvstart = kvend = kvindex;
+
+        maxRec = kvmeta.capacity() / NMETA;
+        softLimit = (int)(kvbuffer.length * spillper);
+        bufferRemaining = softLimit;
+
+        // k/v serialization
+        // k/v serialization
+        keyClass = ConfigUtils.getIntermediateOutputKeyClass(this.conf);
+        valClass = ConfigUtils.getIntermediateOutputValueClass(this.conf);
+        serializationFactory = new SerializationFactory(this.conf);
+        keySerializer = serializationFactory.getSerializer(keyClass);
+        valSerializer = serializationFactory.getSerializer(valClass);
+        valSerializer.open(bb);
+        keySerializer.open(bb);
+
+        spillInProgress = false;
+        minSpillsForCombine = this.conf.getInt(TezRuntimeConfiguration.TEZ_RUNTIME_COMBINE_MIN_SPILLS, 3);
+        spillThread.setDaemon(true);
+        spillThread.setName("SpillThread {DefaultSorterForESS}");
+        spillLock.lock();
+        try {
+            spillThread.start();
+            while (!spillThreadRunning) {
+                spillDone.await();
+            }
+        } catch (InterruptedException e) {
+            //interrupt spill thread
+            spillThread.interrupt();
+            Thread.currentThread().interrupt();
+            throw new IOException("Spill thread failed to initialize", e);
+        } finally {
+            spillLock.unlock();
+        }
+        if (sortSpillException != null) {
+            throw new IOException("Spill thread failed to initialize", sortSpillException);
+        }
+    }
+
+    @VisibleForTesting
+    static int computeSortBufferSize(int availableMemoryMB, String logContext) {
+
+        if (availableMemoryMB <= 0) {
+            throw new RuntimeException(TezRuntimeConfiguration.TEZ_RUNTIME_IO_SORT_MB +
+                "=" + availableMemoryMB + ". It should be > 0");
+        }
+
+        if (availableMemoryMB > MAX_IO_SORT_MB) {
+            LOG.warn(logContext + ": Scaling down " + TezRuntimeConfiguration.TEZ_RUNTIME_IO_SORT_MB +
+                "=" + availableMemoryMB + " to " + MAX_IO_SORT_MB
+                + " (max sort buffer size supported for DefaultSorter)");
+        }
+
+        // cap sort buffer to MAX_IO_SORT_MB for DefaultSorter.
+        // Not using 2047 to avoid any ArrayIndexOutofBounds in collect() phase.
+        return Math.min(MAX_IO_SORT_MB, availableMemoryMB);
+    }
+
+    public void write(Object key, Object value)
+        throws IOException {
+        collect(key, value, partitioner.getPartition(key, value, partitions));
+    }
+
+    /**
+     * Serialize the key, value to intermediate storage.
+     * When this method returns, kvindex must refer to sufficient unused
+     * storage to store one METADATA.
+     */
+    synchronized void collect(Object key, Object value, final int partition
+    ) throws IOException {
+
+        if (key.getClass() != keyClass) {
+            throw new IOException("Type mismatch in key from map: expected "
+                + keyClass.getName() + ", received "
+                + key.getClass().getName());
+        }
+        if (value.getClass() != valClass) {
+            throw new IOException("Type mismatch in value from map: expected "
+                + valClass.getName() + ", received "
+                + value.getClass().getName());
+        }
+        if (partition < 0 || partition >= partitions) {
+            throw new IOException("Illegal partition for " + key + " (" +
+                partition + ")" + ", TotalPartitions: " + partitions);
+        }
+
+        bufferRemaining -= METASIZE;
+        if (bufferRemaining <= 0) {
+            // start spill if the thread is not running and the soft limit has been
+            // reached
+            spillLock.lock();
+            try {
+                do {
+                    if (!spillInProgress) {
+                        final int kvbidx = 4 * kvindex;
+                        final int kvbend = 4 * kvend;
+                        // serialized, unspilled bytes always lie between kvindex and
+                        // bufindex, crossing the equator. Note that any void space
+                        // created by a reset must be included in "used" bytes
+                        final int bUsed = distanceTo(kvbidx, bufindex);
+                        final boolean bufsoftlimit = bUsed >= softLimit;
+                        if ((kvbend + METASIZE) % kvbuffer.length !=
+                            equator - (equator % METASIZE)) {
+                            // spill finished, reclaim space
+                            resetSpill();
+                            bufferRemaining = Math.min(
+                                distanceTo(bufindex, kvbidx) - 2 * METASIZE,
+                                softLimit - bUsed) - METASIZE;
+                            continue;
+                        } else if (bufsoftlimit && kvindex != kvend) {
+                            // spill records, if any collected; check latter, as it may
+                            // be possible for metadata alignment to hit spill pcnt
+                            startSpill();
+                            final int avgRec = (int) (inputByteCounter.get() / inputRecordCounter.get());
+                            // leave at least half the split buffer for serialization data
+                            // ensure that kvindex >= bufindex
+                            final int distkvi = distanceTo(bufindex, kvbidx);
+                            /**
+                             * Reason for capping sort buffer to MAX_IO_SORT_MB
+                             * E.g
+                             * kvbuffer.length = 2146435072 (2047 MB)
+                             * Corner case: bufIndex=2026133899, kvbidx=523629312.
+                             * distkvi = mod - i + j = 2146435072 - 2026133899 + 523629312 = 643930485
+                             * newPos = (2026133899 + (max(.., min(643930485/2, 271128624))) (This would
+                             * overflow)
+                             */
+                            final int newPos = (bufindex +
+                                Math.max(2 * METASIZE - 1,
+                                    Math.min(distkvi / 2,
+                                        distkvi / (METASIZE + avgRec) * METASIZE)))
+                                % kvbuffer.length;
+                            setEquator(newPos);
+                            bufmark = bufindex = newPos;
+                            final int serBound = 4 * kvend;
+                            // bytes remaining before the lock must be held and limits
+                            // checked is the minimum of three arcs: the metadata space, the
+                            // serialization space, and the soft limit
+                            bufferRemaining = Math.min(
+                                // metadata max
+                                distanceTo(bufend, newPos),
+                                Math.min(
+                                    // serialization max
+                                    distanceTo(newPos, serBound),
+                                    // soft limit
+                                    softLimit)) - 2 * METASIZE;
+                        }
+                    }
+                } while (false);
+            } finally {
+                spillLock.unlock();
+            }
+        }
+
+        try {
+            // serialize key bytes into buffer
+            int keystart = bufindex;
+            keySerializer.serialize(key);
+            if (bufindex < keystart) {
+                // wrapped the key; must make contiguous
+                bb.shiftBufferedKey();
+                keystart = 0;
+            }
+            // serialize value bytes into buffer
+            final int valstart = bufindex;
+            valSerializer.serialize(value);
+            // It's possible for records to have zero length, i.e. the serializer
+            // will perform no writes. To ensure that the boundary conditions are
+            // checked and that the kvindex invariant is maintained, perform a
+            // zero-length write into the buffer. The logic monitoring this could be
+            // moved into collect, but this is cleaner and inexpensive. For now, it
+            // is acceptable.
+            bb.write(b0, 0, 0);
+
+            // the record must be marked after the preceding write, as the metadata
+            // for this record are not yet written
+            int valend = bb.markRecord();
+
+            inputRecordCounter.addAndGet(1);
+            inputByteCounter.addAndGet(
+                distanceTo(keystart, valend, bufvoid));
+
+            // write accounting info
+            kvmeta.put(kvindex + PARTITION, partition);
+            kvmeta.put(kvindex + KEYSTART, keystart);
+            kvmeta.put(kvindex + VALSTART, valstart);
+            kvmeta.put(kvindex + VALLEN, distanceTo(valstart, valend));
+            // advance kvindex
+            kvindex = (int)(((long)kvindex - NMETA + kvmeta.capacity()) % kvmeta.capacity());
+            totalKeys++;
+        } catch (SpillBufferTooSmallException e) {
+            LOG.info(": Record too large for in-memory buffer: " + e.getMessage());
+            spillSingleRecord(key, value, partition);
+            return;
+        }
+    }
+
+    /**
+     * Set the point from which meta and serialization data expand. The meta
+     * indices are aligned with the buffer, so metadata never spans the ends of
+     * the circular buffer.
+     */
+    private void setEquator(int pos) {
+        equator = pos;
+        // set index prior to first entry, aligned at meta boundary
+        final int aligned = pos - (pos % METASIZE);
+        // Cast one of the operands to long to avoid integer overflow
+        kvindex = (int) (((long) aligned - METASIZE + kvbuffer.length) % kvbuffer.length) / 4;
+        if (LOG.isInfoEnabled()) {
+            LOG.info(": " + "(EQUATOR) " + pos + " kvi " + kvindex +
+                "(" + (kvindex * 4) + ")");
+        }
+    }
+
+    /**
+     * The spill is complete, so set the buffer and meta indices to be equal to
+     * the new equator to free space for continuing collection. Note that when
+     * kvindex == kvend == kvstart, the buffer is empty.
+     */
+    private void resetSpill() {
+        final int e = equator;
+        bufstart = bufend = e;
+        final int aligned = e - (e % METASIZE);
+        // set start/end to point to first meta record
+        // Cast one of the operands to long to avoid integer overflow
+        kvstart = kvend = (int) (((long) aligned - METASIZE + kvbuffer.length) % kvbuffer.length) / 4;
+    }
+
+    /**
+     * Compute the distance in bytes between two indices in the serialization
+     * buffer.
+     * @see #distanceTo(int,int,int)
+     */
+    final int distanceTo(final int i, final int j) {
+        return distanceTo(i, j, kvbuffer.length);
+    }
+
+    /**
+     * Compute the distance between two indices in the circular buffer given the
+     * max distance.
+     */
+    int distanceTo(final int i, final int j, final int mod) {
+        return i <= j
+            ? j - i
+            : mod - i + j;
+    }
+
+    /**
+     * For the given meta position, return the offset into the int-sized
+     * kvmeta buffer.
+     */
+    int offsetFor(int metapos) {
+        return (metapos % maxRec) * NMETA;
+    }
+
+    /**
+     * Compare logical range, st i, j MOD offset capacity.
+     * Compare by key.
+     * @see IndexedSortable#compare
+     */
+    public int compare(final int mi, final int mj) {
+        final int kvi = offsetFor(mi);
+        final int kvj = offsetFor(mj);
+        // sort by key
+        int result = comparator.compare(kvbuffer,
+            kvmeta.get(kvi + KEYSTART),
+            kvmeta.get(kvi + VALSTART) - kvmeta.get(kvi + KEYSTART),
+            kvbuffer,
+            kvmeta.get(kvj + KEYSTART),
+            kvmeta.get(kvj + VALSTART) - kvmeta.get(kvj + KEYSTART));
+        if (result == 0) {
+            sameKey++;
+        }
+        return result;
+    }
+
+    final byte META_BUFFER_TMP[] = new byte[METASIZE];
+    /**
+     * Swap metadata for items i,j
+     * @see IndexedSortable#swap
+     */
+    public void swap(final int mi, final int mj) {
+        int iOff = (mi % maxRec) * METASIZE;
+        int jOff = (mj % maxRec) * METASIZE;
+        System.arraycopy(kvbuffer, iOff, META_BUFFER_TMP, 0, METASIZE);
+        System.arraycopy(kvbuffer, jOff, kvbuffer, iOff, METASIZE);
+        System.arraycopy(META_BUFFER_TMP, 0, kvbuffer, jOff, METASIZE);
+    }
+
+    /**
+     * Inner class managing the spill of serialized records to disk.
+     */
+    protected class BlockingBuffer extends NonSyncDataOutputStream {
+
+        public BlockingBuffer() {
+            super(new DefaultSorterForRss.Buffer());
+        }
+
+        /**
+         * Mark end of record. Note that this is required if the buffer is to
+         * cut the spill in the proper place.
+         */
+        public int markRecord() {
+            bufmark = bufindex;
+            return bufindex;
+        }
+
+        /**
+         * Set position from last mark to end of writable buffer, then rewrite
+         * the data between last mark and kvindex.
+         * This handles a special case where the key wraps around the buffer.
+         * If the key is to be passed to a RawComparator, then it must be
+         * contiguous in the buffer. This recopies the data in the buffer back
+         * into itself, but starting at the beginning of the buffer. Note that
+         * this method should <b>only</b> be called immediately after detecting
+         * this condition. To call it at any other time is undefined and would
+         * likely result in data loss or corruption.
+         * @see #markRecord()
+         */
+        protected void shiftBufferedKey() throws IOException {
+            // spillLock unnecessary; both kvend and kvindex are current
+            int headbytelen = bufvoid - bufmark;
+            bufvoid = bufmark;
+            final int kvbidx = 4 * kvindex;
+            final int kvbend = 4 * kvend;
+            final int avail =
+                Math.min(distanceTo(0, kvbidx), distanceTo(0, kvbend));
+            if (bufindex + headbytelen < avail) {
+                System.arraycopy(kvbuffer, 0, kvbuffer, headbytelen, bufindex);
+                System.arraycopy(kvbuffer, bufvoid, kvbuffer, 0, headbytelen);
+                bufindex += headbytelen;
+                bufferRemaining -= kvbuffer.length - bufvoid;
+            } else {
+                byte[] keytmp = new byte[bufindex];
+                System.arraycopy(kvbuffer, 0, keytmp, 0, bufindex);
+                bufindex = 0;
+                out.write(kvbuffer, bufmark, headbytelen);
+                out.write(keytmp);
+            }
+        }
+    }
+
+    public class Buffer extends OutputStream {
+        private final byte[] scratch = new byte[1];
+
+        @Override
+        public void write(int v)
+            throws IOException {
+            scratch[0] = (byte)v;
+            write(scratch, 0, 1);
+        }
+
+        /**
+         * Attempt to write a sequence of bytes to the collection buffer.
+         * This method will block if the spill thread is running and it
+         * cannot write.
+         * @throws SpillBufferTooSmallException if record is too large to
+         *    deserialize into the collection buffer.
+         */
+        @Override
+        public void write(byte b[], int off, int len)
+            throws IOException {
+            // must always verify the invariant that at least METASIZE bytes are
+            // available beyond kvindex, even when len == 0
+            bufferRemaining -= len;
+            if (bufferRemaining <= 0) {
+                // writing these bytes could exhaust available buffer space or fill
+                // the buffer to soft limit. check if spill or blocking are necessary
+                boolean blockwrite = false;
+                spillLock.lock();
+                try {
+                    do {
+                        final int kvbidx = 4 * kvindex;
+                        final int kvbend = 4 * kvend;
+                        // ser distance to key index
+                        final int distkvi = distanceTo(bufindex, kvbidx);
+                        // ser distance to spill end index
+                        final int distkve = distanceTo(bufindex, kvbend);
+
+                        // if kvindex is closer than kvend, then a spill is neither in
+                        // progress nor complete and reset since the lock was held. The
+                        // write should block only if there is insufficient space to
+                        // complete the current write, write the metadata for this record,
+                        // and write the metadata for the next record. If kvend is closer,
+                        // then the write should block if there is too little space for
+                        // either the metadata or the current write. Note that collect
+                        // ensures its metadata requirement with a zero-length write
+                        blockwrite = distkvi <= distkve
+                            ? distkvi <= len + 2 * METASIZE
+                            : distkve <= len || distanceTo(bufend, kvbidx) < 2 * METASIZE;
+
+                        if (!spillInProgress) {
+                            if (blockwrite) {
+                                if ((kvbend + METASIZE) % kvbuffer.length !=
+                                    equator - (equator % METASIZE)) {
+                                    // spill finished, reclaim space
+                                    // need to use meta exclusively; zero-len rec & 100% spill
+                                    // pcnt would fail
+                                    resetSpill(); // resetSpill doesn't move bufindex, kvindex
+                                    bufferRemaining = Math.min(
+                                        distkvi - 2 * METASIZE,
+                                        softLimit - distanceTo(kvbidx, bufindex)) - len;
+                                    continue;
+                                }
+                                // we have records we can spill; only spill if blocked
+                                if (kvindex != kvend) {
+                                    startSpill();
+                                    // Blocked on this write, waiting for the spill just
+                                    // initiated to finish. Instead of repositioning the marker
+                                    // and copying the partial record, we set the record start
+                                    // to be the new equator
+                                    setEquator(bufmark);
+                                } else {
+                                    // We have no buffered records, and this record is too large
+                                    // to write into kvbuffer. We must spill it directly from
+                                    // collect
+                                    final int size = distanceTo(bufstart, bufindex) + len;
+                                    setEquator(0);
+                                    bufstart = bufend = bufindex = equator;
+                                    kvstart = kvend = kvindex;
+                                    bufvoid = kvbuffer.length;
+                                    throw new SpillBufferTooSmallException(size + " bytes");
+                                }
+                            }
+                        }
+
+                        if (blockwrite) {
+                            // wait for spill
+                            try {
+                                while (spillInProgress) {
+                                    spillDone.await();
+                                }
+                            } catch (InterruptedException e) {
+                                Thread.currentThread().interrupt();
+                                throw new IOInterruptedException(
+                                    "Buffer interrupted while waiting for the writer", e);
+                            }
+                        }
+                    } while (blockwrite);
+                } finally {
+                    spillLock.unlock();
+                }
+            }
+            // here, we know that we have sufficient space to write
+            // int overflow possible with (bufindex + len)
+            long futureBufIndex = (long) bufindex + len;
+            if (futureBufIndex > bufvoid) {
+                final int gaplen = bufvoid - bufindex;
+                System.arraycopy(b, off, kvbuffer, bufindex, gaplen);
+                len -= gaplen;
+                off += gaplen;
+                bufindex = 0;
+            }
+            System.arraycopy(b, off, kvbuffer, bufindex, len);
+            bufindex += len;
+        }
+    }
+
+    void interruptSpillThread() throws IOException {
+        assert !spillLock.isHeldByCurrentThread();
+        // shut down spill thread and wait for it to exit. Since the preceding
+        // ensures that it is finished with its work (and sortAndSpill did not
+        // throw), we elect to use an interrupt instead of setting a flag.
+        // Spilling simultaneously from this thread while the spill thread
+        // finishes its work might be both a useful way to extend this and also
+        // sufficient motivation for the latter approach.
+        try {
+            spillThread.interrupt();
+            spillThread.join();
+        } catch (InterruptedException e) {
+            LOG.info("Spill thread interrupted");
+            //Reset status
+            Thread.currentThread().interrupt();
+            throw new IOInterruptedException("Spill failed", e);
+        }
+    }
+
+    private synchronized void cleanup() throws IOException {
+        if (!cleanup) {
+            return;
+        }
+        cleanup(spillFilePaths);
+        cleanup(spillFileIndexPaths);
+    }
+
+    private synchronized void cleanup(Map<Integer, Path> spillMap) {
+        for (Map.Entry<Integer, Path> entry : spillMap.entrySet()) {
+            Path path = entry.getValue();
+            if (path != null) {
+                try {
+                    LOG.info("Deleting " + path);
+                    rfs.delete(path, true);
+                } catch (IOException ioe) {
+                    LOG.warn("Error in deleting " + path);
+                }
+            }
+        }
+    }
+
+    public void flush() throws IOException {
+        LOG.info("Starting flush of map output");
+        if (Thread.currentThread().isInterrupted()) {
+            /**
+             * Possible that the thread got interrupted when flush was happening or when the flush was
+             * never invoked. As a part of cleanup activity in TezTaskRunner, it would invoke close()
+             * on all I/O. At that time, this is safe to cleanup
+             */
+            if (cleanup) {
+                cleanup();
+            }
+            try {
+                interruptSpillThread();
+            } catch(IOException e) {
+                //safe to ignore
+            }
+            return;
+        }
+
+        spillLock.lock();
+        try {
+            while (spillInProgress) {
+                spillDone.await();
+            }
+
+            final int kvbend = 4 * kvend;
+            if ((kvbend + METASIZE) % kvbuffer.length !=
+                equator - (equator % METASIZE)) {
+                // spill finished
+                resetSpill();
+            }
+            if (kvindex != kvend) {
+                kvend = (kvindex + NMETA) % kvmeta.capacity();
+                bufend = bufmark;
+                if (LOG.isInfoEnabled()) {
+                    LOG.info("Sorting & Spilling map output. "
+                        + "bufstart = " + bufstart + ", bufend = " + bufmark + ", bufvoid = " + bufvoid
+                        + "; " + "kvstart=" + kvstart + "(" + (kvstart * 4) + ")"
+                        + ", kvend = " + kvend + "(" + (kvend * 4) + ")"
+                        + ", length = " + (distanceTo(kvend, kvstart, kvmeta.capacity()) + 1) + "/" +
+                        maxRec);
+                }
+                long sameKeyCount = 0;
+                long totalKeysCount = 0;
+                synchronized (this) {
+                    sameKeyCount = sameKey;
+                    totalKeysCount = totalKeys;
+                }
+                sortAndSpill(sameKeyCount, totalKeysCount);
+            }
+        } catch (InterruptedException e) {
+            //Reset status
+            Thread.currentThread().interrupt();
+            interruptSpillThread();
+            throw new IOException("Interrupted while waiting for the writer", e);
+        } finally {
+            spillLock.unlock();
+        }
+
+        interruptSpillThread();
+
+        try {
+            mergeParts();
+        } catch (InterruptedException e) {
+            Thread.currentThread().interrupt();
+        }
+    }
+
+    public void close() throws IOException {
+        this.spillFileIndexPaths.clear();
+        this.spillFilePaths.clear();
+        kvbuffer = null;
+        kvmeta = null;
+    }
+
+    protected class SpillThread extends Thread {
+
+        volatile long totalKeysCount;
+        volatile long sameKeyCount;
+
+        @Override
+        public void run() {
+            spillLock.lock();
+            try {
+                spillThreadRunning = true;
+                while (true) {
+                    spillDone.signal();
+                    while (!spillInProgress) {
+                        spillReady.await();
+                    }
+                    try {
+                        spillLock.unlock();
+                        sortAndSpill(sameKeyCount, totalKeysCount);
+                    } catch (Throwable t) {
+                        LOG.warn(": " + "Got an exception in sortAndSpill", t);
+                        sortSpillException = t;
+                    } finally {
+                        spillLock.lock();
+                        if (bufend < bufstart) {
+                            bufvoid = kvbuffer.length;
+                        }
+                        kvstart = kvend;
+                        bufstart = bufend;
+                        spillInProgress = false;
+                    }
+                }
+            } catch (InterruptedException e) {
+                LOG.info("Spill thread interrupted");
+                Thread.currentThread().interrupt();
+            } finally {
+                spillLock.unlock();
+                spillThreadRunning = false;
+            }
+        }
+
+        public void setTotalKeysProcessed(long sameKeyCount, long totalKeysCount) {
+            this.sameKeyCount = sameKeyCount;
+            this.totalKeysCount = totalKeysCount;
+        }
+    }
+
+    private void startSpill() {
+        assert !spillInProgress;
+        kvend = (kvindex + NMETA) % kvmeta.capacity();
+        bufend = bufmark;
+        spillInProgress = true;
+        if (LOG.isInfoEnabled()) {
+            LOG.info(": Spilling map output."
+                + "bufstart=" + bufstart + ", bufend = " + bufmark + ", bufvoid = " + bufvoid
+                +"; kvstart=" + kvstart + "(" + (kvstart * 4) + ")"
+                +", kvend = " + kvend + "(" + (kvend * 4) + ")"
+                + ", length = " + (distanceTo(kvend, kvstart, kvmeta.capacity()) + 1) + "/" + maxRec);
+        }
+        spillThread.setTotalKeysProcessed(sameKey, totalKeys);
+        spillReady.signal();
+    }
+
+    int getMetaStart() {
+        return kvend / NMETA;
+    }
+
+    int getMetaEnd() {
+        return 1 + // kvend is a valid record
+            (kvstart >= kvend
+                ? kvstart
+                : kvmeta.capacity() + kvstart) / NMETA;
+    }
+
+    private boolean isRLENeeded(long sameKeyCount, long totalKeysCount) {
+        return (sameKeyCount > (0.1 * totalKeysCount)) || (sameKeyCount < 0);
+    }
+
+    protected void sortAndSpill(long sameKeyCount, long totalKeysCount)
+        throws IOException, InterruptedException {
+        final int mstart = getMetaStart();
+        final int mend = getMetaEnd();
+        sorter.sort(this, mstart, mend, () -> {});
+        spill(mstart, mend, sameKeyCount, totalKeysCount);
+    }
+
+    protected void spill(int mstart, int mend, long sameKeyCount, long totalKeysCount)
+        throws IOException, InterruptedException {
+
+        //approximate the length of the output file to be the length of the
+        //buffer + header lengths for the partitions
+        final long size = (bufend >= bufstart
+            ? bufend - bufstart
+            : (bufvoid - bufend) + bufstart) +
+            partitions * APPROX_HEADER_LENGTH;
+        FSDataOutputStream out = null;
+        try {
+            // create spill file
+            final TezSpillRecord spillRec = new TezSpillRecord(partitions);
+            final Path filename =
+                shuffleReadOutput.getSpillFileForWrite(numSpills, size);
+            spillFilePaths.put(numSpills, filename);
+            out = rfs.create(filename);
+            if (!SPILL_FILE_PERMS.equals(SPILL_FILE_PERMS.applyUMask(FsPermission.getUMask(conf)))) {
+                rfs.setPermission(filename, SPILL_FILE_PERMS);
+            }
+
+            int spindex = mstart;
+            final DefaultSorterForRss.InMemValBytes value = createInMemValBytes();
+            boolean rle = isRLENeeded(sameKeyCount, totalKeysCount);
+            for (int i = 0; i < partitions; ++i) {
+                IFile.Writer writer = null;
+                try {
+                    long segmentStart = out.getPos();
+                    if (spindex < mend && kvmeta.get(offsetFor(spindex) + PARTITION) == i) {
+                        writer = new IFile.Writer(conf, out, keyClass, valClass, codec,
+                            null, null, rle);
+                    }
+                    // spill directly
+                    DataInputBuffer key = new DataInputBuffer();
+                    while (spindex < mend &&
+                        kvmeta.get(offsetFor(spindex) + PARTITION) == i) {
+                        final int kvoff = offsetFor(spindex);
+                        int keystart = kvmeta.get(kvoff + KEYSTART);
+                        int valstart = kvmeta.get(kvoff + VALSTART);
+                        key.reset(kvbuffer, keystart, valstart - keystart);
+                        getVBytesForOffset(kvoff, value);
+                        writer.append(key, value);
+                        ++spindex;
+                    }
+
+                    long rawLength = 0;
+                    long partLength = 0;
+                    // close the writer
+                    if (writer != null) {
+                        writer.close();
+                        rawLength = writer.getRawLength();
+                        partLength = writer.getCompressedLength();
+                    }
+                    // record offsets
+                    final TezIndexRecord rec =
+                        new TezIndexRecord(segmentStart, rawLength, partLength);
+                    spillRec.putIndex(rec, i);
+                    writer = null;
+                } finally {
+                    if (null != writer) writer.close();
+                }
+            }
+
+            if (totalIndexCacheMemory >= indexCacheMemoryLimit) {
+                // create spill index file
+                Path indexFilename =
+                    shuffleReadOutput.getSpillIndexFileForWrite(numSpills, partitions
+                        * MAP_OUTPUT_INDEX_RECORD_LENGTH);
+                spillFileIndexPaths.put(numSpills, indexFilename);
+                spillRec.writeToFile(indexFilename, conf);
+            } else {
+                indexCacheList.add(spillRec);
+                totalIndexCacheMemory +=
+                    spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH;
+            }
+            LOG.info("Finished spill " + numSpills + " at " + filename.toString());
+            ++numSpills;
+        } finally {
+            if (out != null) out.close();
+        }
+    }
+
+    /**
+     * Handles the degenerate case where serialization fails to fit in
+     * the in-memory buffer, so we must spill the record from collect
+     * directly to a spill file. Consider this "losing".
+     */
+    private void spillSingleRecord(final Object key, final Object value,
+                                   int partition) throws IOException {
+        long size = kvbuffer.length + partitions * APPROX_HEADER_LENGTH;
+        FSDataOutputStream out = null;
+        try {
+            // create spill file
+            final TezSpillRecord spillRec = new TezSpillRecord(partitions);
+            final Path filename =
+                shuffleReadOutput.getSpillFileForWrite(numSpills, size);
+            spillFilePaths.put(numSpills, filename);
+            out = rfs.create(filename);
+            if (!SPILL_FILE_PERMS.equals(SPILL_FILE_PERMS.applyUMask(FsPermission.getUMask(conf)))) {
+                rfs.setPermission(filename, SPILL_FILE_PERMS);
+            }
+
+            // we don't run the combiner for a single record
+            for (int i = 0; i < partitions; ++i) {
+                IFile.Writer writer = null;
+                try {
+                    long segmentStart = out.getPos();
+                    // Create a new codec, don't care!
+                    if (i == partition) {
+                        writer = new IFile.Writer(conf, out, keyClass, valClass, codec,
+                            null, null, false);
+                    }
+                    if (i == partition) {
+                        final long recordStart = out.getPos();
+                        writer.append(key, value);
+                        // Note that our map byte count will not be accurate with
+                        // compression
+                        inputByteCounter.addAndGet(out.getPos() - recordStart);
+                    }
+                    long rawLength =0;
+                    long partLength =0;
+                    if (writer != null) {
+                        writer.close();
+                        rawLength = writer.getRawLength();
+                        partLength = writer.getCompressedLength();
+                    }
+
+                    // record offsets
+                    TezIndexRecord rec = new TezIndexRecord(segmentStart, rawLength, partLength);
+                    spillRec.putIndex(rec, i);
+
+                    writer = null;
+                } catch (IOException e) {
+                    if (null != writer) writer.close();
+                    throw e;
+                }
+            }
+            if (totalIndexCacheMemory >= indexCacheMemoryLimit) {
+                // create spill index file
+                Path indexFilename =
+                    shuffleReadOutput.getSpillIndexFileForWrite(numSpills, partitions
+                        * MAP_OUTPUT_INDEX_RECORD_LENGTH);
+                spillFileIndexPaths.put(numSpills, indexFilename);
+                spillRec.writeToFile(indexFilename, conf);
+            } else {
+                indexCacheList.add(spillRec);
+                totalIndexCacheMemory +=
+                    spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH;
+            }
+            ++numSpills;
+        } finally {
+            if (out != null) out.close();
+        }
+    }
+
+    protected int getInMemVBytesLength(int kvoff) {
+        // get the keystart for the next serialized value to be the end
+        // of this value. If this is the last value in the buffer, use bufend
+        final int vallen = kvmeta.get(kvoff + VALLEN);
+        assert vallen >= 0;
+        return vallen;
+    }
+
+    /**
+     * Given an offset, populate vbytes with the associated set of
+     * deserialized value bytes. Should only be called during a spill.
+     */
+    int getVBytesForOffset(int kvoff, DefaultSorterForRss.InMemValBytes vbytes) {
+        int vallen = getInMemVBytesLength(kvoff);
+        vbytes.reset(kvbuffer, kvmeta.get(kvoff + VALSTART), vallen);
+        return vallen;
+    }
+
+    /**
+     * Inner class wrapping valuebytes, used for appendRaw.
+     */
+    static class InMemValBytes extends DataInputBuffer {
+        private byte[] buffer;
+        private int start;
+        private int length;
+        private final int bufvoid;
+
+        public InMemValBytes(int bufvoid) {
+            this.bufvoid = bufvoid;
+        }
+
+        public void reset(byte[] buffer, int start, int length) {
+            this.buffer = buffer;
+            this.start = start;
+            this.length = length;
+
+            if (start + length > bufvoid) {
+                this.buffer = new byte[this.length];
+                final int taillen = bufvoid - start;
+                System.arraycopy(buffer, start, this.buffer, 0, taillen);
+                System.arraycopy(buffer, 0, this.buffer, taillen, length-taillen);
+                this.start = 0;
+            }
+
+            super.reset(this.buffer, this.start, this.length);
+        }
+    }
+
+    DefaultSorterForRss.InMemValBytes createInMemValBytes() {
+        return new DefaultSorterForRss.InMemValBytes(bufvoid);
+    }
+
+    private void mergeParts() throws IOException, InterruptedException {
+        final Path[] filename = new Path[numSpills];
+
+        for(int i = 0; i < numSpills; i++) {
+            filename[i] = spillFilePaths.get(i);
+        }
+
+        // read in paged indices
+        for (int i = indexCacheList.size(); i < numSpills; ++i) {
+            Path indexFileName = spillFileIndexPaths.get(i);
+            indexCacheList.add(new TezSpillRecord(indexFileName, conf));
+        }
+
+        if (numSpills != 0) {
+            for (int parts = 0; parts < partitions; parts++) {
+                //create the segments to be merged
+                List<TezMerger.Segment> segmentList =
+                    new ArrayList<TezMerger.Segment>(numSpills);
+                for (int i = 0; i < numSpills; i++) {
+                    TezIndexRecord indexRecord = indexCacheList.get(i).getIndex(parts);
+                    TezMerger.DiskSegment s =
+                        new TezMerger.DiskSegment(rfs, filename[i], indexRecord.getStartOffset(),
+                            indexRecord.getPartLength(), codec, ifileReadAhead,
+                            ifileReadAheadLength, ifileBufferSize, true);
+                    segmentList.add(s);
+                }
+
+                int mergeFactor =
+                    this.conf.getInt(TezRuntimeConfiguration.TEZ_RUNTIME_IO_SORT_FACTOR,
+                        TezRuntimeConfiguration.TEZ_RUNTIME_IO_SORT_FACTOR_DEFAULT);
+                // sort the segments only if there are intermediate merges
+                boolean sortSegments = segmentList.size() > mergeFactor;
+                //merge and set TezRawKeyValueIterator
+                kvIter = TezMerger.merge(conf, rfs,
+                    keyClass, valClass, codec,
+                    segmentList, mergeFactor,
+                    new Path("taskshuffleread"),
+                    (RawComparator) ConfigUtils.getIntermediateInputKeyComparator(conf),
+                    () -> {}, sortSegments, true,
+                    null, null, null,
+                    null);
+            }
+
+        }
+    }
+
+    public TezRawKeyValueIterator getIterator() {
+        return kvIter;
+    }
+
+    public void stop() throws IOException {
+        final Path[] filename = new Path[numSpills];
+        for(int i = 0; i < numSpills; i++) {
+            filename[i] = spillFilePaths.get(i);
+        }
+        for(int i = 0; i < numSpills; i++) {
+            rfs.delete(filename[i],true);
+        }
+    }
+
+    private static class EmptyIterator implements TezRawKeyValueIterator {
+        final Progress progress = new Progress();
+
+        EmptyIterator() {
+            progress.set(1.0f);
+        }
+
+        @Override
+        public DataInputBuffer getKey() throws IOException {
+            throw new RuntimeException("No keys on an empty iterator");
+        }
+
+        @Override
+        public DataInputBuffer getValue() throws IOException {
+            throw new RuntimeException("No values on an empty iterator");
+        }
+
+        @Override
+        public boolean next() throws IOException {
+            return false;
+        }
+
+        @Override
+        public boolean hasNext() throws IOException {
+            return false;
+        }
+
+        @Override
+        public void close() throws IOException {
+        }
+
+        @Override
+        public Progress getProgress() {
+            return progress;
+        }
+
+        @Override
+        public boolean isSameKey() throws IOException {
+            throw new UnsupportedOperationException("isSameKey is not supported");
+        }
+    }
+
+    /**
+     * Exception indicating that the allocated sort buffer is insufficient to hold
+     * the current record.
+     */
+    @SuppressWarnings("serial")
+    public static class SpillBufferTooSmallException extends IOException {
+        public SpillBufferTooSmallException(String s) {
+            super(s);
+        }
+    }
+}
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/writers/RssPartitionedKVWriter.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/writers/RssPartitionedKVWriter.java
new file mode 100644
index 000000000..2704c170d
--- /dev/null
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/writers/RssPartitionedKVWriter.java
@@ -0,0 +1,200 @@
+package org.apache.tez.runtime.library.common.writers;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Lists;
+import com.google.protobuf.ByteString;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.serializer.SerializationFactory;
+import org.apache.hadoop.io.serializer.Serializer;
+import org.apache.tez.common.TezCommonUtils;
+import org.apache.tez.common.TezUtilsInternal;
+import org.apache.tez.common.counters.TaskCounter;
+import org.apache.tez.common.counters.TezCounter;
+import org.apache.tez.dag.api.TezConfiguration;
+import org.apache.tez.runtime.api.Event;
+import org.apache.tez.runtime.api.OutputContext;
+import org.apache.tez.runtime.api.events.CompositeDataMovementEvent;
+import org.apache.tez.runtime.library.api.KeyValuesWriter;
+import org.apache.tez.runtime.library.api.Partitioner;
+import org.apache.tez.runtime.library.common.ConfigUtils;
+import org.apache.tez.runtime.library.common.TezRuntimeUtils;
+import org.apache.tez.runtime.library.common.shuffle.ShuffleUtils;
+import org.apache.tez.runtime.library.common.sort.impl.PipelinedSorterForRss;
+import org.apache.tez.runtime.library.shuffle.impl.ShuffleUserPayloads;
+import org.apache.tez.shuffle.ess.EssShuffleManager;
+import org.apache.tez.shuffle.ess.EssShuffleWriter;
+
+import java.io.IOException;
+import java.io.OutputStream;
+import java.nio.ByteBuffer;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.zip.Deflater;
+
+public class RssPartitionedKVWriter extends KeyValuesWriter {
+    private final EssShuffleWriter essShuffleWriter;
+    private final OutputContext outputContext;
+    private final Configuration conf;
+    private final Partitioner partitioner;
+    private final int numPartitions;
+    private final long[] sizePerPartition;
+    private int[] numRecordsPerPartition;
+    private final Deflater deflater = TezCommonUtils.newBestCompressionDeflater();
+    private final TezCounter bytesCounter;
+    private final TezCounter recordsCounter;
+
+
+    private final Class keyClass;
+    private final Class valClass;
+    private final SerializationFactory serializationFactory;
+    private final Serializer keySerializer;
+    private final Serializer valSerializer;
+    private final ByteBuffer kvBuffer;
+
+    public RssPartitionedKVWriter(
+        EssShuffleManager essShuffleManager,
+        OutputContext outputContext,
+        Configuration conf,
+        int numPartitions) throws IOException {
+        this.outputContext = outputContext;
+        this.conf = conf;
+        this.numPartitions = numPartitions;
+        String vertexUniqueId = outputContext.getDagIdentifier() + "-" +
+            outputContext.getTaskVertexName();
+        essShuffleWriter = essShuffleManager.getWriter(
+            outputContext.getTaskAttemptNumber(),
+            vertexUniqueId.hashCode(),
+            outputContext.getTaskIndex(),
+            outputContext.getVertexParallelism(),
+            numPartitions,
+            ConfigUtils.getIntermediateOutputKeyClass(this.conf),
+            ConfigUtils.getIntermediateOutputValueClass(this.conf)
+        );
+        this.partitioner = TezRuntimeUtils.instantiatePartitioner(this.conf);
+        this.sizePerPartition = new long[numPartitions];
+        this.numRecordsPerPartition = new int[numPartitions];
+        bytesCounter = outputContext.getCounters().findCounter(TaskCounter.OUTPUT_BYTES);
+        recordsCounter = outputContext.getCounters().findCounter(TaskCounter.OUTPUT_RECORDS);
+
+        keyClass = ConfigUtils.getIntermediateOutputKeyClass(this.conf);
+        valClass = ConfigUtils.getIntermediateOutputValueClass(this.conf);
+        serializationFactory = new SerializationFactory(this.conf);
+        keySerializer = serializationFactory.getSerializer(keyClass);
+        valSerializer = serializationFactory.getSerializer(valClass);
+        kvBuffer = ByteBuffer.allocate(20 * 1024 * 1024);
+        BufferStreamWrapper out = new BufferStreamWrapper(kvBuffer);
+        keySerializer.open(out);
+        valSerializer.open(out);
+    }
+
+    private static class BufferStreamWrapper extends OutputStream
+    {
+        private final ByteBuffer out;
+        public BufferStreamWrapper(ByteBuffer out) {
+            this.out = out;
+        }
+
+        @Override
+        public void write(int b) throws IOException { out.put((byte)b); }
+        @Override
+        public void write(byte[] b) throws IOException { out.put(b); }
+        @Override
+        public void write(byte[] b, int off, int len) throws IOException { out.put(b, off, len); }
+    }
+
+    @Override
+    public void write(Object key, Iterable<Object> values) throws IOException {
+        Iterator<Object> it = values.iterator();
+        while (it.hasNext()) {
+            write(key, it.next());
+        }
+    }
+
+    @Override
+    public void write(Object key, Object value) throws IOException {
+        int partitionId = partitioner.getPartition(key, value, numPartitions);
+        kvBuffer.clear();
+        keySerializer.serialize(key);
+        int keyLength = kvBuffer.position();
+        valSerializer.serialize(value);
+        int valLength = kvBuffer.position() - keyLength;
+        int bytesWrittern = essShuffleWriter.write(kvBuffer.array(), keyLength, valLength, partitioner.getPartition(
+            key, value, numPartitions));
+        sizePerPartition[partitionId] += bytesWrittern;
+        numRecordsPerPartition[partitionId]++;
+        bytesCounter.increment(bytesWrittern);
+        recordsCounter.increment(1);
+    }
+
+    public List<Event> close() throws IOException {
+        essShuffleWriter.close();
+        List<Event> events = Lists.newLinkedList();
+        events.add(generateVMEvent());
+        events.add(generateDMEvent());
+        return events;
+    }
+
+    private Event generateVMEvent() throws IOException {
+        return ShuffleUtils.generateVMEvent(outputContext, this.sizePerPartition,
+            false, deflater);
+    }
+
+    private Event generateDMEvent() throws IOException {
+        BitSet emptyPartitions = getEmptyPartitions(numRecordsPerPartition);
+        return generateDMEvent(false, -1, false, outputContext.getUniqueIdentifier(), emptyPartitions);
+    }
+
+    private Event generateDMEvent(boolean addSpillDetails, int spillId,
+                                  boolean isLastSpill, String pathComponent, BitSet emptyPartitions)
+        throws IOException {
+
+        outputContext.notifyProgress();
+        ShuffleUserPayloads.DataMovementEventPayloadProto.Builder payloadBuilder = ShuffleUserPayloads.DataMovementEventPayloadProto
+            .newBuilder();
+
+        String host = outputContext.getExecutionContext().getHostName();
+        if (emptyPartitions.cardinality() != 0) {
+            // Empty partitions exist
+            ByteString emptyPartitionsByteString =
+                TezCommonUtils.compressByteArrayToByteString(TezUtilsInternal.toByteArray
+                    (emptyPartitions), deflater);
+            payloadBuilder.setEmptyPartitions(emptyPartitionsByteString);
+        }
+
+        if (emptyPartitions.cardinality() != numPartitions) {
+            // Populate payload only if at least 1 partition has data
+            payloadBuilder.setHost(host);
+            payloadBuilder.setPort(getShufflePort());
+            payloadBuilder.setPathComponent(pathComponent);
+        }
+
+        if (addSpillDetails) {
+            payloadBuilder.setSpillId(spillId);
+            payloadBuilder.setLastEvent(isLastSpill);
+        }
+
+        ByteBuffer payload = payloadBuilder.build().toByteString().asReadOnlyByteBuffer();
+        return CompositeDataMovementEvent.create(0, numPartitions, payload);
+    }
+
+    int getShufflePort() throws IOException {
+        String auxiliaryService = conf.get(TezConfiguration.TEZ_AM_SHUFFLE_AUXILIARY_SERVICE_ID,
+            TezConfiguration.TEZ_AM_SHUFFLE_AUXILIARY_SERVICE_ID_DEFAULT);
+        ByteBuffer shuffleMetadata = outputContext
+            .getServiceProviderMetaData(auxiliaryService);
+        int shufflePort = ShuffleUtils.deserializeShuffleProviderMetaData(shuffleMetadata);
+        return shufflePort;
+    }
+
+    private BitSet getEmptyPartitions(int[] recordsPerPartition) {
+        Preconditions.checkArgument(recordsPerPartition != null, "records per partition can not be null");
+        BitSet emptyPartitions = new BitSet();
+        for (int i = 0; i < numPartitions; i++) {
+            if (recordsPerPartition[i] == 0 ) {
+                emptyPartitions.set(i);
+            }
+        }
+        return emptyPartitions;
+    }
+}
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/conf/OrderedGroupedKVInputConfig.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/conf/OrderedGroupedKVInputConfig.java
index 11a8d6fd9..ac36650b1 100644
--- a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/conf/OrderedGroupedKVInputConfig.java
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/conf/OrderedGroupedKVInputConfig.java
@@ -35,8 +35,10 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.CommonConfigurationKeys;
 import org.apache.tez.common.TezUtils;
 import org.apache.tez.dag.api.UserPayload;
+import org.apache.tez.runtime.ess.RssShuffleManagerFactory;
 import org.apache.tez.runtime.library.api.TezRuntimeConfiguration;
 import org.apache.tez.runtime.library.common.ConfigUtils;
+import org.apache.tez.runtime.library.input.RssOrderedGroupedKVInput;
 import org.apache.tez.runtime.library.input.OrderedGroupedKVInput;
 import org.apache.tez.runtime.library.input.OrderedGroupedInputLegacy;
 
@@ -240,6 +242,8 @@ public class OrderedGroupedKVInputConfig {
     this.conf = conf;
     if (useLegacyInput) {
       inputClassName = OrderedGroupedInputLegacy.class.getName();
+    } else if (RssShuffleManagerFactory.isRssEnabled(conf)) {
+      inputClassName = RssOrderedGroupedKVInput.class.getName();
     } else {
       inputClassName = OrderedGroupedKVInput.class.getName();
     }
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/conf/OrderedPartitionedKVEdgeConfig.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/conf/OrderedPartitionedKVEdgeConfig.java
index 350420390..6dae27235 100644
--- a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/conf/OrderedPartitionedKVEdgeConfig.java
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/conf/OrderedPartitionedKVEdgeConfig.java
@@ -27,12 +27,15 @@ import com.google.common.base.Preconditions;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.classification.InterfaceStability;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.tez.common.TezUtils;
 import org.apache.tez.dag.api.EdgeManagerPluginDescriptor;
 import org.apache.tez.dag.api.EdgeProperty;
 import org.apache.tez.dag.api.InputDescriptor;
 import org.apache.tez.dag.api.OutputDescriptor;
 import org.apache.tez.dag.api.UserPayload;
+import org.apache.tez.runtime.ess.RssShuffleManagerFactory;
 import org.apache.tez.runtime.library.output.OrderedPartitionedKVOutput;
+import org.apache.tez.runtime.library.output.RssOrderedPartitionedKVOutput;
 
 /**
  * Configure payloads for the OrderedPartitionedKVOutput and OrderedGroupedKVInput pair </p>
@@ -53,6 +56,7 @@ public class OrderedPartitionedKVEdgeConfig
       OrderedGroupedKVInputConfig inputConfiguration) {
     this.outputConf = outputConfiguration;
     this.inputConf = inputConfiguration;
+
   }
 
   /**
@@ -95,6 +99,9 @@ public class OrderedPartitionedKVEdgeConfig
 
   @Override
   public String getOutputClassName() {
+    if (RssShuffleManagerFactory.isRssEnabled(getOutputPayload())) {
+      return RssOrderedPartitionedKVOutput.class.getName();
+    }
     return OrderedPartitionedKVOutput.class.getName();
   }
 
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/conf/UnorderedPartitionedKVEdgeConfig.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/conf/UnorderedPartitionedKVEdgeConfig.java
index 52da49151..d3a8b1916 100644
--- a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/conf/UnorderedPartitionedKVEdgeConfig.java
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/conf/UnorderedPartitionedKVEdgeConfig.java
@@ -34,7 +34,10 @@ import org.apache.tez.dag.api.EdgeProperty;
 import org.apache.tez.dag.api.InputDescriptor;
 import org.apache.tez.dag.api.OutputDescriptor;
 import org.apache.tez.dag.api.UserPayload;
+import org.apache.tez.runtime.ess.RssShuffleManagerFactory;
+import org.apache.tez.runtime.library.input.RssUnorderedKVInput;
 import org.apache.tez.runtime.library.input.UnorderedKVInput;
+import org.apache.tez.runtime.library.output.RssUnorderedPartitionedKVOutput;
 import org.apache.tez.runtime.library.output.UnorderedPartitionedKVOutput;
 
 /**
@@ -99,6 +102,9 @@ public class UnorderedPartitionedKVEdgeConfig
 
   @Override
   public String getOutputClassName() {
+    if (RssShuffleManagerFactory.isRssEnabled(getOutputPayload())) {
+      return RssUnorderedPartitionedKVOutput.class.getName();
+    }
     return UnorderedPartitionedKVOutput.class.getName();
   }
 
@@ -119,6 +125,9 @@ public class UnorderedPartitionedKVEdgeConfig
 
   @Override
   public String getInputClassName() {
+    if (RssShuffleManagerFactory.isRssEnabled(getInputPayload())) {
+      return RssUnorderedKVInput.class.getName();
+    }
     return UnorderedKVInput.class.getName();
   }
 
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/input/RssOrderedGroupedKVInput.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/input/RssOrderedGroupedKVInput.java
new file mode 100644
index 000000000..2b3cff61e
--- /dev/null
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/input/RssOrderedGroupedKVInput.java
@@ -0,0 +1,244 @@
+package org.apache.tez.runtime.library.input;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.tez.common.TezRuntimeFrameworkConfigs;
+import org.apache.tez.common.TezUtils;
+import org.apache.tez.common.counters.TaskCounter;
+import org.apache.tez.common.counters.TezCounter;
+import org.apache.tez.runtime.api.AbstractLogicalInput;
+import org.apache.tez.runtime.api.Event;
+import org.apache.tez.runtime.api.InputContext;
+import org.apache.tez.runtime.api.events.CompositeRoutedDataMovementEvent;
+import org.apache.tez.runtime.api.events.DataMovementEvent;
+import org.apache.tez.runtime.ess.RssShuffleManagerFactory;
+import org.apache.tez.runtime.library.api.KeyValuesReader;
+import org.apache.tez.runtime.library.api.TezRuntimeConfiguration;
+import org.apache.tez.runtime.library.common.ConfigUtils;
+import org.apache.tez.runtime.library.common.MemoryUpdateCallbackHandler;
+import org.apache.tez.runtime.library.common.ValuesIterator;
+import org.apache.tez.runtime.library.common.shuffle.orderedgrouped.Shuffle;
+import org.apache.tez.runtime.library.common.sort.impl.PipelinedSorterForRss;
+import org.apache.tez.runtime.library.common.sort.impl.TezRawKeyValueIterator;
+import org.apache.tez.runtime.library.partitioner.HashPartitioner;
+import org.apache.tez.shuffle.ess.EssShuffleManager;
+import org.apache.tez.shuffle.ess.EssShuffleReader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import scala.Product4;
+import scala.collection.Iterator;
+
+import java.io.IOException;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+public class RssOrderedGroupedKVInput extends AbstractLogicalInput {
+    static final Logger LOG = LoggerFactory.getLogger(RssOrderedGroupedKVInput.class);
+    private final String appId;
+    private final int shuffleId;
+    private final AtomicBoolean isStarted = new AtomicBoolean(false);
+    private final BlockingQueue<Event> pendingEvents = new LinkedBlockingQueue<Event>();
+
+    private Configuration conf;
+    private PipelinedSorterForRss sorter;
+    private EssShuffleReader reader;
+
+    private TezCounter inputKeyCounter;
+    private TezCounter inputValueCounter;
+
+    private AtomicBoolean knowPartition = new AtomicBoolean(false);
+    private int partitionId = -1;
+    private int partitionCount = -1;
+    private int numInputs = -1;
+
+    TezRawKeyValueIterator _iter;
+
+    public void setNumInputs(int numInputs) {
+        this.numInputs = numInputs;
+    }
+
+    protected MemoryUpdateCallbackHandler memoryUpdateCallbackHandler;
+
+    public RssOrderedGroupedKVInput(InputContext inputContext, int numPhysicalInputs) {
+        super(inputContext, numPhysicalInputs);
+
+        this.appId = inputContext.getApplicationId().toString();
+        String srcVertexUniqueId = inputContext.getDagIdentifier() + "-" +
+            inputContext.getSourceVertexName();
+        this.shuffleId = srcVertexUniqueId.hashCode();
+    }
+
+    @Override
+    public List<Event> initialize() throws Exception {
+        this.conf = TezUtils.createConfFromUserPayload(getContext().getUserPayload());
+
+        long initialMemoryRequest = Shuffle.getInitialMemoryRequirement(conf,
+            getContext().getTotalMemoryAvailableToTask());
+        // max 128M
+        // TODO configurable
+        initialMemoryRequest = Math.min(initialMemoryRequest, 512 * 1024 * 1024);
+        this.memoryUpdateCallbackHandler = new MemoryUpdateCallbackHandler();
+        getContext().requestInitialMemory(initialMemoryRequest, memoryUpdateCallbackHandler);
+
+        this.inputKeyCounter = getContext().getCounters().findCounter(TaskCounter.REDUCE_INPUT_GROUPS);
+        this.inputValueCounter = getContext().getCounters().findCounter(
+                TaskCounter.REDUCE_INPUT_RECORDS);
+
+        return null;
+    }
+
+    @Override
+    public synchronized void handleEvents(List<Event> inputEvents) throws Exception {
+        if (knowPartition.get()) {
+            LOG.info(("Already know partition, just return"));
+            return;
+        }
+        if (!isStarted.get()) {
+            pendingEvents.addAll(inputEvents);
+        } else {
+            doHandleEvents(inputEvents);
+        }
+    }
+
+    private void doHandleEvents(List<Event> events) throws Exception {
+        for (Event event : events) {
+            doHandleEvent(event);
+        }
+    }
+
+    private void doHandleEvent(Event event) throws Exception {
+        if (event instanceof DataMovementEvent) {
+            DataMovementEvent dmEvent = (DataMovementEvent)event;
+            partitionId = dmEvent.getSourceIndex();
+            partitionCount = 1;
+
+            LOG.info("handled DataMovementEvent, partitionId " + partitionId);
+            knowPartition.set(true);
+            getContext().inputIsReady();
+        } else if (event instanceof CompositeRoutedDataMovementEvent) {
+            CompositeRoutedDataMovementEvent crdme = (CompositeRoutedDataMovementEvent)event;
+            partitionId = crdme.getSourceIndex();
+            partitionCount = crdme.getCount();
+
+            LOG.info("handled CompositeRoutedDataMovementEvent, partitionId " + partitionId + " partitionCount " + partitionCount);
+            knowPartition.set(true);
+            getContext().inputIsReady();
+        } else {
+            LOG.warn("Unknown event! " + event);
+        }
+    }
+
+    @Override
+    public List<Event> close() throws Exception {
+        LOG.info("inside close, set to null");
+        if (sorter != null) {
+            sorter.close();
+        }
+        if (_iter != null) {
+            _iter.close();
+        }
+        sorter = null;
+        _iter = null;
+        return null;
+    }
+
+    @Override
+    public synchronized void start() throws Exception {
+        if (!isStarted.get()) {
+            isStarted.set(true);
+            List<Event> pending = new LinkedList<Event>();
+            pendingEvents.drainTo(pending);
+            if (pending.size() > 0) {
+                doHandleEvents(pending);
+            }
+        }
+    }
+
+    @Override
+    public long getReservedMemory() {
+        return memoryUpdateCallbackHandler.getMemoryAssigned();
+    }
+
+    @Override
+    public KeyValuesReader getReader() throws Exception {
+        this.conf = TezUtils.createConfFromUserPayload(getContext().getUserPayload());
+        LOG.info("Codec " + conf.get(TezRuntimeConfiguration.TEZ_RUNTIME_COMPRESS_CODEC));
+        this.conf.set(TezRuntimeConfiguration.TEZ_RUNTIME_PARTITIONER_CLASS, HashPartitioner.class.getName());
+        this.conf.setStrings(TezRuntimeFrameworkConfigs.LOCAL_DIRS, getContext().getWorkDirs());
+        //conf.set("tez.runtime.compress.codec", "org.apache.hadoop.io.compress.SnappyCodec");
+        LOG.info("memory assigned " + memoryUpdateCallbackHandler.getMemoryAssigned() + ", numInputs " + numInputs);
+        long memory = memoryUpdateCallbackHandler.getMemoryAssigned() / 1024 / 1024;
+        if (memory <= 0) {
+            memory = 10;
+        }
+        this.sorter = new PipelinedSorterForRss(getContext(), conf, partitionCount,  (long) (memory * 0.8));
+        LOG.info("before get reader, partitionId " + partitionId + ", partitionCount " + partitionCount);
+        EssShuffleManager essShuffleManager = RssShuffleManagerFactory.
+            getEssShuffleManager(appId, conf);
+        reader = essShuffleManager.getReader(
+            getContext().getTaskAttemptNumber(),
+            shuffleId,
+            partitionId,
+            partitionId + partitionCount,
+            false
+        );
+        Iterator<Product4> iter = reader.read();
+        while (iter.hasNext()) {
+            Product4 product4 = iter.next();
+            sorter.write((int) product4._1(), (int) product4._2(), (byte[]) product4._3(), (byte[]) product4._4());
+        }
+        // call reader's close
+        reader.close();
+        reader = null;
+
+        sorter.flush();
+        _iter = sorter.getKvIter();
+        RawComparator rawComparator = ConfigUtils.getIntermediateOutputKeyComparator(conf);
+        Class<?> keyClass = ConfigUtils.getIntermediateOutputKeyClass(conf);
+        Class<?> valClass = ConfigUtils.getIntermediateOutputValueClass(conf);
+        LOG.info(getContext().getSourceVertexName() + ": " + "creating ValuesIterator with "
+                + "comparator=" + rawComparator.getClass().getName()
+                + ", keyClass=" + keyClass.getName()
+                + ", valClass=" + valClass.getName());
+
+        if (_iter != null) {
+            ValuesIterator vIter = new ValuesIterator(_iter, rawComparator, keyClass, valClass, conf, inputKeyCounter, inputValueCounter);
+            return new KeyValuesReader() {
+                @Override
+                public boolean next() throws IOException {
+                    return vIter.moveToNext();
+                }
+
+                @Override
+                public Object getCurrentKey() throws IOException {
+                    return vIter.getKey();
+                }
+
+                @Override
+                public Iterable<Object> getCurrentValues() throws IOException {
+                    return vIter.getValues();
+                }
+            };
+        } else {
+            return new KeyValuesReader() {
+                @Override
+                public boolean next() throws IOException {
+                    return false;
+                }
+
+                @Override
+                public Object getCurrentKey() throws IOException {
+                    return null;
+                }
+
+                @Override
+                public Iterable<Object> getCurrentValues() throws IOException {
+                    return null;
+                }
+            };
+        }
+    }
+}
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/input/RssUnorderedKVInput.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/input/RssUnorderedKVInput.java
new file mode 100644
index 000000000..827337231
--- /dev/null
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/input/RssUnorderedKVInput.java
@@ -0,0 +1,203 @@
+package org.apache.tez.runtime.library.input;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.io.serializer.Deserializer;
+import org.apache.hadoop.io.serializer.SerializationFactory;
+import org.apache.tez.common.TezRuntimeFrameworkConfigs;
+import org.apache.tez.common.TezUtils;
+import org.apache.tez.common.counters.TaskCounter;
+import org.apache.tez.common.counters.TezCounter;
+import org.apache.tez.runtime.api.AbstractLogicalInput;
+import org.apache.tez.runtime.api.Event;
+import org.apache.tez.runtime.api.InputContext;
+import org.apache.tez.runtime.api.events.CompositeRoutedDataMovementEvent;
+import org.apache.tez.runtime.api.events.DataMovementEvent;
+import org.apache.tez.runtime.ess.RssShuffleManagerFactory;
+import org.apache.tez.runtime.library.api.KeyValueReader;
+import org.apache.tez.runtime.library.api.TezRuntimeConfiguration;
+import org.apache.tez.runtime.library.common.ConfigUtils;
+import org.apache.tez.runtime.library.common.MemoryUpdateCallbackHandler;
+import org.apache.tez.runtime.library.common.shuffle.orderedgrouped.Shuffle;
+import org.apache.tez.runtime.library.partitioner.HashPartitioner;
+import org.apache.tez.shuffle.ess.EssShuffleManager;
+import org.apache.tez.shuffle.ess.EssShuffleReader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import scala.Product4;
+import scala.collection.Iterator;
+
+import java.io.IOException;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+public class RssUnorderedKVInput extends AbstractLogicalInput {
+    static final Logger LOG = LoggerFactory.getLogger(RssUnorderedKVInput.class);
+    private final String appId;
+    private final int shuffleId;
+    private final AtomicBoolean isStarted = new AtomicBoolean(false);
+    private final BlockingQueue<Event> pendingEvents = new LinkedBlockingQueue<Event>();
+
+    private Configuration conf;
+    private EssShuffleReader reader;
+    private TezCounter inputRecordCounter;
+
+    private AtomicBoolean knowPartition = new AtomicBoolean(false);
+    private int partitionId = -1;
+    private int partitionCount = -1;
+
+    protected MemoryUpdateCallbackHandler memoryUpdateCallbackHandler;
+
+    public RssUnorderedKVInput(InputContext inputContext, int numPhysicalInputs) {
+        super(inputContext, numPhysicalInputs);
+
+        this.appId = inputContext.getApplicationId().toString();
+        String srcVertexUniqueId = inputContext.getDagIdentifier() + "-" +
+            inputContext.getSourceVertexName();
+        this.shuffleId = srcVertexUniqueId.hashCode();
+    }
+
+    @Override
+    public List<Event> initialize() throws Exception {
+        this.conf = TezUtils.createConfFromUserPayload(getContext().getUserPayload());
+        LOG.info("Codec " + conf.get(TezRuntimeConfiguration.TEZ_RUNTIME_COMPRESS_CODEC));
+        long initialMemoryRequest = Shuffle.getInitialMemoryRequirement(conf,
+            getContext().getTotalMemoryAvailableToTask());
+        // max 128M
+        initialMemoryRequest = Math.min(initialMemoryRequest, 128 * 1024 * 1024);
+        this.memoryUpdateCallbackHandler = new MemoryUpdateCallbackHandler();
+        getContext().requestInitialMemory((initialMemoryRequest), memoryUpdateCallbackHandler);
+
+        this.inputRecordCounter = getContext().getCounters().findCounter(
+                TaskCounter.INPUT_RECORDS_PROCESSED);
+
+        return null;
+    }
+
+    @Override
+    public synchronized void handleEvents(List<Event> inputEvents) throws Exception {
+        if (knowPartition.get()) {
+            LOG.info(("Already know partition, just return"));
+            return;
+        }
+        if (!isStarted.get()) {
+            pendingEvents.addAll(inputEvents);
+        } else {
+            doHandleEvents(inputEvents);
+        }
+    }
+
+    private void doHandleEvents(List<Event> events) throws Exception {
+        for (Event event : events) {
+            doHandleEvent(event);
+        }
+    }
+
+    private void doHandleEvent(Event event) throws Exception {
+        if (event instanceof DataMovementEvent) {
+            DataMovementEvent dmEvent = (DataMovementEvent)event;
+            partitionId = dmEvent.getSourceIndex();
+            partitionCount = 1;
+
+            LOG.info("handled DataMovementEvent, partitionId " + partitionId);
+            knowPartition.set(true);
+            getContext().inputIsReady();
+        } else if (event instanceof CompositeRoutedDataMovementEvent) {
+            CompositeRoutedDataMovementEvent crdme = (CompositeRoutedDataMovementEvent)event;
+            partitionId = crdme.getSourceIndex();
+            partitionCount = crdme.getCount();
+
+            LOG.info("handled CompositeRoutedDataMovementEvent, partitionId " + partitionId + " partitionCount " + partitionCount);
+            knowPartition.set(true);
+            getContext().inputIsReady();
+        } else {
+            LOG.warn("Unknown event! " + event);
+        }
+    }
+
+    @Override
+    public long getReservedMemory() {
+        // TODO configurable
+        return 64 * 1024 * 1024;
+    }
+
+    @Override
+    public List<Event> close() throws Exception {
+        LOG.info("inside close, set to null");
+        reader.close();
+        reader = null;
+        return null;
+    }
+
+    @Override
+    public synchronized void start() throws Exception {
+        if (!isStarted.get()) {
+            isStarted.set(true);
+            List<Event> pending = new LinkedList<Event>();
+            pendingEvents.drainTo(pending);
+            if (pending.size() > 0) {
+                doHandleEvents(pending);
+            }
+        }
+    }
+
+    @Override
+    public KeyValueReader getReader() throws Exception {
+        this.conf = TezUtils.createConfFromUserPayload(getContext().getUserPayload());
+        this.conf.set(TezRuntimeConfiguration.TEZ_RUNTIME_PARTITIONER_CLASS, HashPartitioner.class.getName());
+        this.conf.setStrings(TezRuntimeFrameworkConfigs.LOCAL_DIRS, getContext().getWorkDirs());
+        EssShuffleManager essShuffleManager = RssShuffleManagerFactory.
+            getEssShuffleManager(appId, conf);
+        reader = essShuffleManager.getReader(
+            getContext().getTaskAttemptNumber(),
+            shuffleId,
+            partitionId,
+            partitionId + partitionCount,
+            false
+        );
+
+        DataInputBuffer keyIn = new DataInputBuffer();
+        DataInputBuffer valueIn = new DataInputBuffer();
+        Class<?> keyClass = ConfigUtils.getIntermediateOutputKeyClass(conf);
+        Class<?> valClass = ConfigUtils.getIntermediateOutputValueClass(conf);
+        SerializationFactory serializationFactory = new SerializationFactory(conf);
+        Deserializer keyDeserializer = serializationFactory.getDeserializer(keyClass);
+        Deserializer valDeserializer = serializationFactory.getDeserializer(valClass);
+        keyDeserializer.open(keyIn);
+        valDeserializer.open(valueIn);
+
+        Iterator<Product4> iter = reader.read();
+        return new KeyValueReader() {
+            Product4 product4;
+            Object key;
+            Object value;
+            @Override
+            public boolean next() throws IOException{
+                if (iter.hasNext()) {
+                    product4 = iter.next();
+                    inputRecordCounter.increment(1);
+                    keyIn.reset((byte[]) product4._3(), 0, (int) product4._1());
+                    valueIn.reset((byte[]) product4._4(), 0, (int) product4._2());
+                    key = keyDeserializer.deserialize(key);
+                    value = valDeserializer.deserialize(value);
+                    return true;
+                } else {
+                    return false;
+                }
+            }
+
+            @Override
+            public Object getCurrentKey() {
+                return key;
+            }
+
+            @Override
+            public Object getCurrentValue() {
+                return value;
+            }
+        };
+    }
+}
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/output/RssOrderedPartitionedKVOutput.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/output/RssOrderedPartitionedKVOutput.java
new file mode 100644
index 000000000..46fd8f889
--- /dev/null
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/output/RssOrderedPartitionedKVOutput.java
@@ -0,0 +1,176 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.tez.runtime.library.output;
+
+import com.aliyun.emr.ess.common.EssConf;
+import com.google.common.base.Preconditions;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceAudience.Public;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.tez.common.TezCommonUtils;
+import org.apache.tez.common.TezRuntimeFrameworkConfigs;
+import org.apache.tez.common.TezUtils;
+import org.apache.tez.common.counters.TaskCounter;
+import org.apache.tez.dag.api.TezConfiguration;
+import org.apache.tez.runtime.api.*;
+import org.apache.tez.runtime.ess.RssShuffleManagerFactory;
+import org.apache.tez.runtime.library.api.TezRuntimeConfiguration;
+import org.apache.tez.runtime.library.common.MemoryUpdateCallbackHandler;
+import org.apache.tez.runtime.library.common.shuffle.ShuffleUtils;
+import org.apache.tez.runtime.library.common.writers.RssPartitionedKVWriter;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.*;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+/**
+ * {@link RssOrderedPartitionedKVOutput} is a {@link LogicalOutput} which can be used to
+ * write Key-Value pairs. The key-value pairs are written to the correct partition based on the
+ * configured Partitioner.
+ */
+@Public
+public class RssOrderedPartitionedKVOutput extends AbstractLogicalOutput {
+
+  private static final Logger LOG = LoggerFactory.getLogger(RssOrderedPartitionedKVOutput.class);
+
+  private Configuration conf;
+  private MemoryUpdateCallbackHandler memoryUpdateCallbackHandler;
+  private RssPartitionedKVWriter kvWriter;
+  private final AtomicBoolean isStarted = new AtomicBoolean(false);
+  private long requestMemory;
+
+  public RssOrderedPartitionedKVOutput(OutputContext outputContext, int numPhysicalOutputs) {
+    super(outputContext, numPhysicalOutputs);
+  }
+
+  @Override
+  public synchronized List<Event> initialize() throws Exception {
+    this.conf = TezUtils.createConfFromUserPayload(getContext().getUserPayload());
+    LOG.info("Codec " + conf.get(TezRuntimeConfiguration.TEZ_RUNTIME_COMPRESS_CODEC));
+    this.conf.setStrings(TezRuntimeFrameworkConfigs.LOCAL_DIRS, getContext().getWorkDirs());
+    this.conf.setInt(TezRuntimeFrameworkConfigs.TEZ_RUNTIME_NUM_EXPECTED_PARTITIONS,
+        getNumPhysicalOutputs());
+    this.memoryUpdateCallbackHandler = new MemoryUpdateCallbackHandler();
+    EssConf essConf = RssShuffleManagerFactory.getEssShuffleManager(getContext().getApplicationId().toString(), conf)
+        .getConf();
+    long sendBufferSize = EssConf.essPushDataBufferSize(essConf);
+    // TODO configurable
+    requestMemory = (long) (sendBufferSize * getNumPhysicalOutputs() * 1.2);
+    if (requestMemory <= 0) {
+      requestMemory = 10 * 1024 * 1024;
+    }
+    LOG.info("request memory size " + requestMemory);
+    getContext().requestInitialMemory(requestMemory, memoryUpdateCallbackHandler);
+    return Collections.emptyList();
+  }
+
+  @Override
+  public synchronized void start() throws Exception {
+    if (!isStarted.get()) {
+      // TODO uncomment
+//      memoryUpdateCallbackHandler.validateUpdateReceived();
+      this.kvWriter = new RssPartitionedKVWriter(
+         RssShuffleManagerFactory.getEssShuffleManager(getContext().getApplicationId().toString(), conf),
+         getContext(),
+         conf,
+         getNumPhysicalOutputs()
+      );
+      isStarted.set(true);
+    }
+  }
+
+  @Override
+  public synchronized Writer getWriter() throws Exception {
+    Preconditions.checkState(isStarted.get(), "Cannot get writer before starting the Output");
+    return kvWriter;
+  }
+
+  @Override
+  public void handleEvents(List<Event> outputEvents) {
+  }
+
+  @Override
+  public long getReservedMemory() {
+    return requestMemory;
+  }
+
+  @Override
+  public synchronized List<Event> close() throws Exception {
+    List<Event> returnEvents = null;
+    if (isStarted.get()) {
+      returnEvents = kvWriter.close();
+      kvWriter = null;
+    } else {
+      LOG.warn(getContext().getDestinationVertexName() +
+          ": Attempting to close output {} of type {} before it was started. Generating empty events",
+          getContext().getDestinationVertexName(), this.getClass().getSimpleName());
+      returnEvents = new LinkedList<Event>();
+      ShuffleUtils
+          .generateEventsForNonStartedOutput(returnEvents, getNumPhysicalOutputs(), getContext(),
+              false, true, TezCommonUtils.newBestCompressionDeflater());
+    }
+
+    // This works for non-started outputs since new counters will be created with an initial value of 0
+    long outputSize = getContext().getCounters().findCounter(TaskCounter.OUTPUT_BYTES).getValue();
+    getContext().getStatisticsReporter().reportDataSize(outputSize);
+    long outputRecords = getContext().getCounters()
+        .findCounter(TaskCounter.OUTPUT_RECORDS).getValue();
+    getContext().getStatisticsReporter().reportItemsProcessed(outputRecords);
+
+    return returnEvents;
+  }
+
+  private static final Set<String> confKeys = new HashSet<String>();
+
+  static {
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_READAHEAD);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_READAHEAD_BYTES);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_IO_FILE_BUFFER_SIZE);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_INDEX_CACHE_MEMORY_LIMIT_BYTES);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_UNORDERED_OUTPUT_BUFFER_SIZE_MB);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_UNORDERED_OUTPUT_MAX_PER_BUFFER_SIZE_BYTES);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_PARTITIONER_CLASS);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_KEY_CLASS);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_VALUE_CLASS);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_COMPRESS);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_COMPRESS_CODEC);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_EMPTY_PARTITION_INFO_VIA_EVENTS_ENABLED);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_CONVERT_USER_PAYLOAD_TO_HISTORY_TEXT);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_PIPELINED_SHUFFLE_ENABLED);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_ENABLE_FINAL_MERGE_IN_OUTPUT);
+    confKeys.add(TezConfiguration.TEZ_COUNTERS_MAX);
+    confKeys.add(TezConfiguration.TEZ_COUNTERS_GROUP_NAME_MAX_LENGTH);
+    confKeys.add(TezConfiguration.TEZ_COUNTERS_COUNTER_NAME_MAX_LENGTH);
+    confKeys.add(TezConfiguration.TEZ_COUNTERS_MAX_GROUPS);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_CLEANUP_FILES_ON_INTERRUPT);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_REPORT_PARTITION_STATS);
+    confKeys.add(TezConfiguration.TEZ_AM_SHUFFLE_AUXILIARY_SERVICE_ID);
+    confKeys.add(
+        TezRuntimeConfiguration.TEZ_RUNTIME_UNORDERED_PARTITIONED_KVWRITER_BUFFER_MERGE_PERCENT);
+  }
+
+  // TODO Maybe add helper methods to extract keys
+  // TODO Maybe add constants or an Enum to access the keys
+
+  @InterfaceAudience.Private
+  public static Set<String> getConfigurationKeySet() {
+    return Collections.unmodifiableSet(confKeys);
+  }
+}
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/output/RssUnorderedPartitionedKVOutput.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/output/RssUnorderedPartitionedKVOutput.java
new file mode 100644
index 000000000..763e9fb14
--- /dev/null
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/output/RssUnorderedPartitionedKVOutput.java
@@ -0,0 +1,176 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.tez.runtime.library.output;
+
+import com.aliyun.emr.ess.common.EssConf;
+import com.google.common.base.Preconditions;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceAudience.Public;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.tez.common.TezCommonUtils;
+import org.apache.tez.common.TezRuntimeFrameworkConfigs;
+import org.apache.tez.common.TezUtils;
+import org.apache.tez.common.counters.TaskCounter;
+import org.apache.tez.dag.api.TezConfiguration;
+import org.apache.tez.runtime.api.*;
+import org.apache.tez.runtime.ess.RssShuffleManagerFactory;
+import org.apache.tez.runtime.library.api.TezRuntimeConfiguration;
+import org.apache.tez.runtime.library.common.MemoryUpdateCallbackHandler;
+import org.apache.tez.runtime.library.common.shuffle.ShuffleUtils;
+import org.apache.tez.runtime.library.common.writers.RssPartitionedKVWriter;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.*;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+/**
+ * {@link RssUnorderedPartitionedKVOutput} is a {@link LogicalOutput} which can be used to
+ * write Key-Value pairs. The key-value pairs are written to the correct partition based on the
+ * configured Partitioner.
+ */
+// TODO code refactor against RssOrderedPartitionedKVOutput
+@Public
+public class RssUnorderedPartitionedKVOutput extends AbstractLogicalOutput {
+
+  private static final Logger LOG = LoggerFactory.getLogger(RssUnorderedPartitionedKVOutput.class);
+
+  private Configuration conf;
+  private MemoryUpdateCallbackHandler memoryUpdateCallbackHandler;
+  private RssPartitionedKVWriter kvWriter;
+  private final AtomicBoolean isStarted = new AtomicBoolean(false);
+  private long requestMemory;
+
+  public RssUnorderedPartitionedKVOutput(OutputContext outputContext, int numPhysicalOutputs) {
+    super(outputContext, numPhysicalOutputs);
+  }
+
+  @Override
+  public synchronized List<Event> initialize() throws Exception {
+    this.conf = TezUtils.createConfFromUserPayload(getContext().getUserPayload());
+    LOG.info("Codec " + conf.get(TezRuntimeConfiguration.TEZ_RUNTIME_COMPRESS_CODEC));
+    this.conf.setStrings(TezRuntimeFrameworkConfigs.LOCAL_DIRS, getContext().getWorkDirs());
+    this.conf.setInt(TezRuntimeFrameworkConfigs.TEZ_RUNTIME_NUM_EXPECTED_PARTITIONS,
+        getNumPhysicalOutputs());
+    this.memoryUpdateCallbackHandler = new MemoryUpdateCallbackHandler();
+    EssConf essConf = RssShuffleManagerFactory.getEssShuffleManager(getContext().getApplicationId().toString(), conf)
+        .getConf();
+    long sendBufferSize = EssConf.essPushDataBufferSize(essConf);
+    requestMemory = (long) (sendBufferSize * getNumPhysicalOutputs() * 1.2);
+    if (requestMemory <= 0) {
+      requestMemory = 10 * 1024 * 1024;
+    }
+    LOG.info("request memory size " + requestMemory);
+    getContext().requestInitialMemory(requestMemory, memoryUpdateCallbackHandler);
+    return Collections.emptyList();
+  }
+
+  @Override
+  public synchronized void start() throws Exception {
+    if (!isStarted.get()) {
+      // TODO uncomment
+//      memoryUpdateCallbackHandler.validateUpdateReceived();
+      this.kvWriter = new RssPartitionedKVWriter(
+         RssShuffleManagerFactory.getEssShuffleManager(getContext().getApplicationId().toString(), conf),
+         getContext(),
+         conf,
+         getNumPhysicalOutputs()
+      );
+      isStarted.set(true);
+    }
+  }
+
+  @Override
+  public synchronized Writer getWriter() throws Exception {
+    Preconditions.checkState(isStarted.get(), "Cannot get writer before starting the Output");
+    return kvWriter;
+  }
+
+  @Override
+  public void handleEvents(List<Event> outputEvents) {
+  }
+
+  @Override
+  public long getReservedMemory() {
+    return requestMemory;
+  }
+
+  @Override
+  public synchronized List<Event> close() throws Exception {
+    List<Event> returnEvents = null;
+    if (isStarted.get()) {
+      returnEvents = kvWriter.close();
+      kvWriter = null;
+    } else {
+      LOG.warn(getContext().getDestinationVertexName() +
+          ": Attempting to close output {} of type {} before it was started. Generating empty events",
+          getContext().getDestinationVertexName(), this.getClass().getSimpleName());
+      returnEvents = new LinkedList<Event>();
+      ShuffleUtils
+          .generateEventsForNonStartedOutput(returnEvents, getNumPhysicalOutputs(), getContext(),
+              false, true, TezCommonUtils.newBestCompressionDeflater());
+    }
+
+    // This works for non-started outputs since new counters will be created with an initial value of 0
+    long outputSize = getContext().getCounters().findCounter(TaskCounter.OUTPUT_BYTES).getValue();
+    getContext().getStatisticsReporter().reportDataSize(outputSize);
+    long outputRecords = getContext().getCounters()
+        .findCounter(TaskCounter.OUTPUT_RECORDS).getValue();
+    getContext().getStatisticsReporter().reportItemsProcessed(outputRecords);
+
+    return returnEvents;
+  }
+
+  private static final Set<String> confKeys = new HashSet<String>();
+
+  static {
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_READAHEAD);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_IFILE_READAHEAD_BYTES);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_IO_FILE_BUFFER_SIZE);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_INDEX_CACHE_MEMORY_LIMIT_BYTES);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_UNORDERED_OUTPUT_BUFFER_SIZE_MB);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_UNORDERED_OUTPUT_MAX_PER_BUFFER_SIZE_BYTES);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_PARTITIONER_CLASS);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_KEY_CLASS);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_VALUE_CLASS);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_COMPRESS);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_COMPRESS_CODEC);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_EMPTY_PARTITION_INFO_VIA_EVENTS_ENABLED);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_CONVERT_USER_PAYLOAD_TO_HISTORY_TEXT);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_PIPELINED_SHUFFLE_ENABLED);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_ENABLE_FINAL_MERGE_IN_OUTPUT);
+    confKeys.add(TezConfiguration.TEZ_COUNTERS_MAX);
+    confKeys.add(TezConfiguration.TEZ_COUNTERS_GROUP_NAME_MAX_LENGTH);
+    confKeys.add(TezConfiguration.TEZ_COUNTERS_COUNTER_NAME_MAX_LENGTH);
+    confKeys.add(TezConfiguration.TEZ_COUNTERS_MAX_GROUPS);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_CLEANUP_FILES_ON_INTERRUPT);
+    confKeys.add(TezRuntimeConfiguration.TEZ_RUNTIME_REPORT_PARTITION_STATS);
+    confKeys.add(TezConfiguration.TEZ_AM_SHUFFLE_AUXILIARY_SERVICE_ID);
+    confKeys.add(
+        TezRuntimeConfiguration.TEZ_RUNTIME_UNORDERED_PARTITIONED_KVWRITER_BUFFER_MERGE_PERCENT);
+  }
+
+  // TODO Maybe add helper methods to extract keys
+  // TODO Maybe add constants or an Enum to access the keys
+
+  @InterfaceAudience.Private
+  public static Set<String> getConfigurationKeySet() {
+    return Collections.unmodifiableSet(confKeys);
+  }
+}
diff --git a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/resources/WeightedScalingMemoryDistributor.java b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/resources/WeightedScalingMemoryDistributor.java
index c5b4fb0f9..a9701c099 100644
--- a/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/resources/WeightedScalingMemoryDistributor.java
+++ b/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/resources/WeightedScalingMemoryDistributor.java
@@ -24,6 +24,9 @@ import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
 
+import org.apache.tez.runtime.library.input.*;
+import org.apache.tez.runtime.library.output.RssOrderedPartitionedKVOutput;
+import org.apache.tez.runtime.library.output.RssUnorderedPartitionedKVOutput;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.classification.InterfaceAudience.Private;
@@ -34,9 +37,6 @@ import org.apache.tez.dag.api.TezConfiguration;
 import org.apache.tez.runtime.common.resources.InitialMemoryAllocator;
 import org.apache.tez.runtime.common.resources.InitialMemoryRequestContext;
 import org.apache.tez.runtime.common.resources.InitialMemoryRequestContext.ComponentType;
-import org.apache.tez.runtime.library.input.OrderedGroupedKVInput;
-import org.apache.tez.runtime.library.input.OrderedGroupedInputLegacy;
-import org.apache.tez.runtime.library.input.UnorderedKVInput;
 import org.apache.tez.runtime.library.output.OrderedPartitionedKVOutput;
 import org.apache.tez.runtime.library.output.UnorderedPartitionedKVOutput;
 
@@ -65,7 +65,7 @@ public class WeightedScalingMemoryDistributor implements InitialMemoryAllocator
   static final double MAX_ADDITIONAL_RESERVATION_FRACTION_PER_IO = 0.1d;
   static final double RESERVATION_FRACTION_PER_IO = 0.015d;
   static final String[] DEFAULT_TASK_MEMORY_WEIGHTED_RATIOS =
-      generateWeightStrings(1, 1, 1, 12, 12, 1, 1);
+      generateWeightStrings(12, 12, 1, 1, 1, 1, 12, 12, 1, 1);
 
   private Configuration conf;
 
@@ -75,7 +75,7 @@ public class WeightedScalingMemoryDistributor implements InitialMemoryAllocator
   @Private
   @VisibleForTesting
   public enum RequestType {
-    PARTITIONED_UNSORTED_OUTPUT, UNSORTED_INPUT, UNSORTED_OUTPUT, SORTED_OUTPUT,
+    RSS_OUTPUT, RSS_SORTED_INPUT, RSS_UNSORTED_INPUT, PARTITIONED_UNSORTED_OUTPUT, UNSORTED_INPUT, UNSORTED_OUTPUT, SORTED_OUTPUT,
     SORTED_MERGED_INPUT, PROCESSOR, OTHER
   };
 
@@ -242,6 +242,13 @@ public class WeightedScalingMemoryDistributor implements InitialMemoryAllocator
       requestType = RequestType.UNSORTED_INPUT;
     } else if (className.equals(UnorderedPartitionedKVOutput.class.getName())) {
       requestType = RequestType.PARTITIONED_UNSORTED_OUTPUT;
+    } else if (className.equals(RssOrderedPartitionedKVOutput.class.getName())
+        || className.equals(RssUnorderedPartitionedKVOutput.class.getName())) {
+      requestType = RequestType.RSS_OUTPUT;
+    } else if (className.equals(RssOrderedGroupedKVInput.class.getName())) {
+      requestType = RequestType.RSS_SORTED_INPUT;
+    } else if (className.equals(RssUnorderedKVInput.class.getName())) {
+      requestType = RequestType.RSS_UNSORTED_INPUT;
     } else {
       requestType = RequestType.OTHER;
       if (LOG.isDebugEnabled()) {
@@ -320,16 +327,19 @@ public class WeightedScalingMemoryDistributor implements InitialMemoryAllocator
     return reserveFraction;
   }
 
-  public static String[] generateWeightStrings(int unsortedPartitioned, int unsorted,
-      int broadcastIn, int sortedOut, int scatterGatherShuffleIn, int proc, int other) {
+  public static String[] generateWeightStrings(int rssPartitionedOutput, int rssSortedInput, int rssUnsortedInput,
+             int unsortedPartitioned, int unsorted, int broadcastIn, int sortedOut, int scatterGatherShuffleIn, int proc, int other) {
     String[] weights = new String[RequestType.values().length];
-    weights[0] = RequestType.PARTITIONED_UNSORTED_OUTPUT.name() + ":" + unsortedPartitioned;
-    weights[1] = RequestType.UNSORTED_OUTPUT.name() + ":" + unsorted;
-    weights[2] = RequestType.UNSORTED_INPUT.name() + ":" + broadcastIn;
-    weights[3] = RequestType.SORTED_OUTPUT.name() + ":" + sortedOut;
-    weights[4] = RequestType.SORTED_MERGED_INPUT.name() + ":" + scatterGatherShuffleIn;
-    weights[5] = RequestType.PROCESSOR.name() + ":" + proc;
-    weights[6] = RequestType.OTHER.name() + ":" + other;
+    weights[0] = RequestType.RSS_OUTPUT.name() + ":" + rssPartitionedOutput;
+    weights[1] = RequestType.RSS_SORTED_INPUT.name() + ":" + rssSortedInput;
+    weights[2] = RequestType.RSS_UNSORTED_INPUT.name() + ":" + rssUnsortedInput;
+    weights[3] = RequestType.PARTITIONED_UNSORTED_OUTPUT.name() + ":" + unsortedPartitioned;
+    weights[4] = RequestType.UNSORTED_OUTPUT.name() + ":" + unsorted;
+    weights[5] = RequestType.UNSORTED_INPUT.name() + ":" + broadcastIn;
+    weights[6] = RequestType.SORTED_OUTPUT.name() + ":" + sortedOut;
+    weights[7] = RequestType.SORTED_MERGED_INPUT.name() + ":" + scatterGatherShuffleIn;
+    weights[8] = RequestType.PROCESSOR.name() + ":" + proc;
+    weights[9] = RequestType.OTHER.name() + ":" + other;
     return weights;
   }
 
diff --git a/tez-runtime-library/src/test/java/org/apache/tez/runtime/common/resources/TestWeightedScalingMemoryDistributor.java b/tez-runtime-library/src/test/java/org/apache/tez/runtime/common/resources/TestWeightedScalingMemoryDistributor.java
index 2fbe26443..f6eaf9383 100644
--- a/tez-runtime-library/src/test/java/org/apache/tez/runtime/common/resources/TestWeightedScalingMemoryDistributor.java
+++ b/tez-runtime-library/src/test/java/org/apache/tez/runtime/common/resources/TestWeightedScalingMemoryDistributor.java
@@ -57,7 +57,7 @@ public class TestWeightedScalingMemoryDistributor extends TestMemoryDistributor
   public void testSimpleWeightedScaling() throws TezException {
     Configuration conf = new Configuration(this.conf);
     conf.setStrings(TezConfiguration.TEZ_TASK_SCALE_MEMORY_WEIGHTED_RATIOS,
-        WeightedScalingMemoryDistributor.generateWeightStrings(0, 0, 1, 2, 3, 1, 1));
+        WeightedScalingMemoryDistributor.generateWeightStrings(0, 0, 0, 0, 0, 1, 2, 3, 1, 1));
     System.err.println(Joiner.on(",").join(conf.getStringCollection(
         TezConfiguration.TEZ_TASK_SCALE_MEMORY_WEIGHTED_RATIOS)));
 
@@ -104,7 +104,7 @@ public class TestWeightedScalingMemoryDistributor extends TestMemoryDistributor
   public void testAdditionalReserveFractionWeightedScaling() throws TezException {
     Configuration conf = new Configuration(this.conf);
     conf.setStrings(TezConfiguration.TEZ_TASK_SCALE_MEMORY_WEIGHTED_RATIOS,
-        WeightedScalingMemoryDistributor.generateWeightStrings(0, 0, 2, 3, 6, 1, 1));
+        WeightedScalingMemoryDistributor.generateWeightStrings(0, 0, 0, 0, 0, 2, 3, 6, 1, 1));
     conf.setDouble(TezConfiguration.TEZ_TASK_SCALE_MEMORY_ADDITIONAL_RESERVATION_FRACTION_PER_IO, 0.025d);
     conf.setDouble(TezConfiguration.TEZ_TASK_SCALE_MEMORY_ADDITIONAL_RESERVATION_FRACTION_MAX, 0.2d);
 
@@ -154,7 +154,7 @@ public class TestWeightedScalingMemoryDistributor extends TestMemoryDistributor
     conf.setBoolean(TezConfiguration.TEZ_TASK_SCALE_MEMORY_NON_CONCURRENT_INPUTS_ENABLED, true);
     conf.setDouble(TezConfiguration.TEZ_TASK_SCALE_MEMORY_RESERVE_FRACTION, 0.2);
     conf.setStrings(TezConfiguration.TEZ_TASK_SCALE_MEMORY_WEIGHTED_RATIOS,
-        WeightedScalingMemoryDistributor.generateWeightStrings(0, 0, 1, 2, 3, 1, 1));
+        WeightedScalingMemoryDistributor.generateWeightStrings(0, 0, 0, 0, 0, 1, 2, 3, 1, 1));
     System.err.println(Joiner.on(",").join(conf.getStringCollection(
         TezConfiguration.TEZ_TASK_SCALE_MEMORY_WEIGHTED_RATIOS)));
 
@@ -210,7 +210,7 @@ public class TestWeightedScalingMemoryDistributor extends TestMemoryDistributor
     conf.setBoolean(TezConfiguration.TEZ_TASK_SCALE_MEMORY_INPUT_OUTPUT_CONCURRENT, false);
     conf.setBoolean(TezConfiguration.TEZ_TASK_SCALE_MEMORY_NON_CONCURRENT_INPUTS_ENABLED, true);
     conf.setStrings(TezConfiguration.TEZ_TASK_SCALE_MEMORY_WEIGHTED_RATIOS,
-        WeightedScalingMemoryDistributor.generateWeightStrings(0, 0, 2, 3, 6, 1, 1));
+        WeightedScalingMemoryDistributor.generateWeightStrings(0, 0, 0, 0, 0, 2, 3, 6, 1, 1));
     conf.setDouble(TezConfiguration.TEZ_TASK_SCALE_MEMORY_ADDITIONAL_RESERVATION_FRACTION_PER_IO, 0.025d);
     conf.setDouble(TezConfiguration.TEZ_TASK_SCALE_MEMORY_ADDITIONAL_RESERVATION_FRACTION_MAX, 0.2d);
 
@@ -260,7 +260,7 @@ public class TestWeightedScalingMemoryDistributor extends TestMemoryDistributor
     conf.setBoolean(TezConfiguration.TEZ_TASK_SCALE_MEMORY_NON_CONCURRENT_INPUTS_ENABLED, false);
     conf.setDouble(TezConfiguration.TEZ_TASK_SCALE_MEMORY_RESERVE_FRACTION, 0.2);
     conf.setStrings(TezConfiguration.TEZ_TASK_SCALE_MEMORY_WEIGHTED_RATIOS,
-        WeightedScalingMemoryDistributor.generateWeightStrings(0, 0, 1, 2, 3, 1, 1));
+        WeightedScalingMemoryDistributor.generateWeightStrings(0, 0, 0, 0, 0, 1, 2, 3, 1, 1));
     System.err.println(Joiner.on(",").join(conf.getStringCollection(
         TezConfiguration.TEZ_TASK_SCALE_MEMORY_WEIGHTED_RATIOS)));
 
